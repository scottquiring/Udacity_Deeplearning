{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 18:\n",
      "Image - Min Value: 25 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGhM0ANAY4IeABr735Itt65De7bWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f188e3e6b38>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 18\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "# def to_hsi(x):\n",
    "#     r = x[:,:,:,0]\n",
    "#     g = x[:,:,:,1]\n",
    "#     b = x[:,:,:,2]\n",
    "    \n",
    "#     theta = np.acos((0.5 * ((r-g) + (r-b))) / (((r-g)**2) + (r-b *)))\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    x = x / 255.0\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i,:,:,:] = color.rgb2hsv(x[i,:,:,:])\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    m = (x.shape[0] if hasattr(x, 'shape') else len(x))\n",
    "    rv = np.zeros((m, 10))\n",
    "    rv[range(m),x] = 1\n",
    "    return rv\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 3.35 s, total: 19.7 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    tensor_shape = tuple([None] + list(image_shape))\n",
    "    return tf.placeholder(tf.float32, shape=tensor_shape, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None,n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def neural_net_learn_rate_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='learn_rate')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    conv_ksize   = list(conv_ksize)\n",
    "    conv_strides = list(conv_strides)\n",
    "    pool_ksize   = list(pool_ksize)\n",
    "    pool_strides = list(pool_strides)\n",
    "    \n",
    "    conv_num_inputs = int(x_tensor.shape[3])\n",
    "    conv_weights = tf.Variable(tf.truncated_normal(\n",
    "        conv_ksize + [conv_num_inputs,conv_num_outputs], stddev=0.01))\n",
    "    conv_strides = [1] + conv_strides + [1]\n",
    "    \n",
    "    rv = tf.nn.conv2d(x_tensor, conv_weights, conv_strides, 'SAME')\n",
    "    \n",
    "    conv_bias = tf.Variable(tf.zeros(rv.shape[1:]))\n",
    "    rv = tf.nn.relu(rv + conv_bias)\n",
    "    \n",
    "    pool_ksize = [1] + pool_ksize + [1]\n",
    "    pool_strides = [1] + pool_strides + [1]\n",
    "    rv = tf.nn.max_pool(rv, pool_ksize, pool_strides, 'SAME')\n",
    "    return rv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    dimensions = [int(x_tensor.shape[1]), num_outputs]\n",
    "    weights = tf.Variable(tf.truncated_normal(dimensions, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.relu(tf.matmul(x_tensor, weights) + bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dimensions = [int(x_tensor.shape[1]), num_outputs]\n",
    "    weights = tf.Variable(tf.truncated_normal(dimensions, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.matmul(x_tensor, weights) + bias\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #with tf.device('/gpu:0'):\n",
    "    x_tensor = x\n",
    "    x_tensor = conv2d_maxpool(x_tensor, 128, [8,8], [1,1], [2,2], [1,1])\n",
    "\n",
    "    x_tensor = conv2d_maxpool(x_tensor, 128, [4,4], [2,2], [2,2], [2,2])\n",
    "    x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "    #with tf.device('/gpu:1'):\n",
    "    x_tensor = conv2d_maxpool(x_tensor,  64, [4,4], [1,1], [2,2], [2,2])\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x_tensor = flatten(x_tensor)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    x_tensor = tf.nn.dropout(fully_conn(x_tensor, 512), keep_prob)\n",
    "    x_tensor = fully_conn(x_tensor, 256)\n",
    "    x_tensor = tf.nn.dropout(fully_conn(x_tensor, 256), keep_prob)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    x_tensor = output(x_tensor, 10)\n",
    "\n",
    "# TODO: return output\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "# x = (neural_net_image_input((32, 32, 3)), neural_net_image_input((32, 32, 3)))\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "learn_rate = neural_net_learn_rate_input()\n",
    "\n",
    "# Model\n",
    "logits = []\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "        logits.append(conv_net(x, keep_prob))\n",
    "\n",
    "logits = tf.concat(logits, 0)\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def split(t,n=2):\n",
    "#     if n == 1: return (t,)\n",
    "#     dimSize = int(t.shape[0])\n",
    "#     partSize = dimSize/n\n",
    "#     maxIdx = int(partSize)\n",
    "#     rv = [t[:maxIdx,...]]\n",
    "#     for i in range(n-2):\n",
    "#         myMin = int(maxIdx)\n",
    "#         nextMax = min(dimSize,float(maxIdx)+partSize)\n",
    "#         myMax = int(nextMax)\n",
    "        \n",
    "#         rv.append(t[myMin:myMax,...])\n",
    "#         maxIdx = nextMax\n",
    "\n",
    "#     rv.append(t[int(maxIdx):,...])\n",
    "#     return tuple(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = np.zeros((100,2,1,5))\n",
    "\n",
    "xs = split(x,6)\n",
    "print([x.shape for x in xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability,\n",
    "                         feature_batch, label_batch, epoch=0):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    if epoch < 125:\n",
    "        learning_rate=0.001\n",
    "    elif epoch < 175:\n",
    "        learning_rate=0.0003\n",
    "    elif epoch < 225:\n",
    "        learning_rate=0.0001\n",
    "    else:\n",
    "        learning_rate=0.00003\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch,\n",
    "                                      learn_rate: learning_rate,\n",
    "                                      keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    train_cost, train_acc = session.run((cost, accuracy),\n",
    "                                        feed_dict={x: feature_batch,\n",
    "                                                   y: label_batch,\n",
    "                                                   keep_prob: 1.0})\n",
    "    valid_cost, valid_acc = session.run((cost, accuracy),\n",
    "                                        feed_dict={x: valid_features,\n",
    "                                                   y: valid_labels,\n",
    "                                                   keep_prob: 1.0})\n",
    "    print(\"Training   loss: {0:.02}, accuracy: {1:.02}\".format(train_cost, train_acc))\n",
    "    print(datetime.datetime.now(),\"Validation loss: {0:.02}, accuracy: {1:.02}\".format(valid_cost, valid_acc))\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 250\n",
    "batch_size = 256\n",
    "keep_probability = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "val_accuracy = []\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels, epoch)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        val_accuracy.append(print_stats(sess, batch_features, batch_labels, cost, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-da7687050ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_accuracy' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwIAAALmCAYAAADxHiOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9616bd6668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, axis = plt.subplots(figsize=(13,13))\n",
    "axis.plot(val_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Training   loss: 2.3, accuracy: 0.15\n",
      "2017-05-17 23:32:53.965677 Validation loss: 2.2, accuracy: 0.17\n",
      "Epoch  1, CIFAR-10 Batch 2:  Training   loss: 2.1, accuracy: 0.23\n",
      "2017-05-17 23:32:56.170224 Validation loss: 2.1, accuracy: 0.18\n",
      "Epoch  1, CIFAR-10 Batch 3:  Training   loss: 1.8, accuracy: 0.28\n",
      "2017-05-17 23:32:58.404825 Validation loss: 2.0, accuracy: 0.18\n",
      "Epoch  1, CIFAR-10 Batch 4:  Training   loss: 2.1, accuracy: 0.12\n",
      "2017-05-17 23:33:00.613129 Validation loss: 2.0, accuracy: 0.19\n",
      "Epoch  1, CIFAR-10 Batch 5:  Training   loss: 2.0, accuracy: 0.3\n",
      "2017-05-17 23:33:02.844105 Validation loss: 1.9, accuracy: 0.23\n",
      "Epoch  2, CIFAR-10 Batch 1:  Training   loss: 2.1, accuracy: 0.23\n",
      "2017-05-17 23:33:05.046366 Validation loss: 1.8, accuracy: 0.27\n",
      "Epoch  2, CIFAR-10 Batch 2:  Training   loss: 1.9, accuracy: 0.35\n",
      "2017-05-17 23:33:07.259769 Validation loss: 1.8, accuracy: 0.28\n",
      "Epoch  2, CIFAR-10 Batch 3:  Training   loss: 1.6, accuracy: 0.35\n",
      "2017-05-17 23:33:09.463928 Validation loss: 1.8, accuracy: 0.3\n",
      "Epoch  2, CIFAR-10 Batch 4:  Training   loss: 1.7, accuracy: 0.3\n",
      "2017-05-17 23:33:11.668956 Validation loss: 1.7, accuracy: 0.32\n",
      "Epoch  2, CIFAR-10 Batch 5:  Training   loss: 1.7, accuracy: 0.35\n",
      "2017-05-17 23:33:13.878344 Validation loss: 1.7, accuracy: 0.31\n",
      "Epoch  3, CIFAR-10 Batch 1:  Training   loss: 1.9, accuracy: 0.33\n",
      "2017-05-17 23:33:16.082686 Validation loss: 1.7, accuracy: 0.35\n",
      "Epoch  3, CIFAR-10 Batch 2:  Training   loss: 1.7, accuracy: 0.43\n",
      "2017-05-17 23:33:18.304036 Validation loss: 1.8, accuracy: 0.31\n",
      "Epoch  3, CIFAR-10 Batch 3:  Training   loss: 1.5, accuracy: 0.35\n",
      "2017-05-17 23:33:20.504488 Validation loss: 1.6, accuracy: 0.36\n",
      "Epoch  3, CIFAR-10 Batch 4:  Training   loss: 1.6, accuracy: 0.28\n",
      "2017-05-17 23:33:22.729754 Validation loss: 1.6, accuracy: 0.39\n",
      "Epoch  3, CIFAR-10 Batch 5:  Training   loss: 1.7, accuracy: 0.45\n",
      "2017-05-17 23:33:24.942474 Validation loss: 1.6, accuracy: 0.4\n",
      "Epoch  4, CIFAR-10 Batch 1:  Training   loss: 1.7, accuracy: 0.4\n",
      "2017-05-17 23:33:27.179250 Validation loss: 1.5, accuracy: 0.41\n",
      "Epoch  4, CIFAR-10 Batch 2:  Training   loss: 1.5, accuracy: 0.5\n",
      "2017-05-17 23:33:29.385965 Validation loss: 1.6, accuracy: 0.4\n",
      "Epoch  4, CIFAR-10 Batch 3:  Training   loss: 1.3, accuracy: 0.48\n",
      "2017-05-17 23:33:31.587393 Validation loss: 1.5, accuracy: 0.43\n",
      "Epoch  4, CIFAR-10 Batch 4:  Training   loss: 1.5, accuracy: 0.45\n",
      "2017-05-17 23:33:33.798130 Validation loss: 1.5, accuracy: 0.44\n",
      "Epoch  4, CIFAR-10 Batch 5:  Training   loss: 1.5, accuracy: 0.53\n",
      "2017-05-17 23:33:36.003600 Validation loss: 1.5, accuracy: 0.44\n",
      "Epoch  5, CIFAR-10 Batch 1:  Training   loss: 1.6, accuracy: 0.43\n",
      "2017-05-17 23:33:38.213839 Validation loss: 1.5, accuracy: 0.46\n",
      "Epoch  5, CIFAR-10 Batch 2:  Training   loss: 1.3, accuracy: 0.4\n",
      "2017-05-17 23:33:40.445481 Validation loss: 1.4, accuracy: 0.46\n",
      "Epoch  5, CIFAR-10 Batch 3:  Training   loss: 1.2, accuracy: 0.55\n",
      "2017-05-17 23:33:42.684742 Validation loss: 1.4, accuracy: 0.47\n",
      "Epoch  5, CIFAR-10 Batch 4:  Training   loss: 1.3, accuracy: 0.5\n",
      "2017-05-17 23:33:44.899036 Validation loss: 1.4, accuracy: 0.47\n",
      "Epoch  5, CIFAR-10 Batch 5:  Training   loss: 1.4, accuracy: 0.53\n",
      "2017-05-17 23:33:47.129946 Validation loss: 1.4, accuracy: 0.47\n",
      "Epoch  6, CIFAR-10 Batch 1:  Training   loss: 1.5, accuracy: 0.45\n",
      "2017-05-17 23:33:49.346122 Validation loss: 1.4, accuracy: 0.47\n",
      "Epoch  6, CIFAR-10 Batch 2:  Training   loss: 1.2, accuracy: 0.57\n",
      "2017-05-17 23:33:51.584042 Validation loss: 1.4, accuracy: 0.48\n",
      "Epoch  6, CIFAR-10 Batch 3:  Training   loss: 1.1, accuracy: 0.57\n",
      "2017-05-17 23:33:53.815207 Validation loss: 1.4, accuracy: 0.5\n",
      "Epoch  6, CIFAR-10 Batch 4:  Training   loss: 1.2, accuracy: 0.6\n",
      "2017-05-17 23:33:56.051103 Validation loss: 1.4, accuracy: 0.5\n",
      "Epoch  6, CIFAR-10 Batch 5:  Training   loss: 1.3, accuracy: 0.58\n",
      "2017-05-17 23:33:58.257509 Validation loss: 1.4, accuracy: 0.52\n",
      "Epoch  7, CIFAR-10 Batch 1:  Training   loss: 1.4, accuracy: 0.57\n",
      "2017-05-17 23:34:00.479053 Validation loss: 1.4, accuracy: 0.51\n",
      "Epoch  7, CIFAR-10 Batch 2:  Training   loss: 1.1, accuracy: 0.62\n",
      "2017-05-17 23:34:02.689083 Validation loss: 1.3, accuracy: 0.51\n",
      "Epoch  7, CIFAR-10 Batch 3:  Training   loss: 0.95, accuracy: 0.65\n",
      "2017-05-17 23:34:04.899800 Validation loss: 1.3, accuracy: 0.51\n",
      "Epoch  7, CIFAR-10 Batch 4:  Training   loss: 1.1, accuracy: 0.6\n",
      "2017-05-17 23:34:07.110968 Validation loss: 1.3, accuracy: 0.53\n",
      "Epoch  7, CIFAR-10 Batch 5:  Training   loss: 1.2, accuracy: 0.57\n",
      "2017-05-17 23:34:09.334351 Validation loss: 1.3, accuracy: 0.53\n",
      "Epoch  8, CIFAR-10 Batch 1:  Training   loss: 1.2, accuracy: 0.55\n",
      "2017-05-17 23:34:11.542474 Validation loss: 1.3, accuracy: 0.52\n",
      "Epoch  8, CIFAR-10 Batch 2:  Training   loss: 1.0, accuracy: 0.6\n",
      "2017-05-17 23:34:13.751169 Validation loss: 1.4, accuracy: 0.52\n",
      "Epoch  8, CIFAR-10 Batch 3:  Training   loss: 0.9, accuracy: 0.62\n",
      "2017-05-17 23:34:15.962937 Validation loss: 1.3, accuracy: 0.54\n",
      "Epoch  8, CIFAR-10 Batch 4:  Training   loss: 0.98, accuracy: 0.68\n",
      "2017-05-17 23:34:18.172533 Validation loss: 1.3, accuracy: 0.53\n",
      "Epoch  8, CIFAR-10 Batch 5:  Training   loss: 1.1, accuracy: 0.57\n",
      "2017-05-17 23:34:20.411693 Validation loss: 1.3, accuracy: 0.55\n",
      "Epoch  9, CIFAR-10 Batch 1:  Training   loss: 1.2, accuracy: 0.55\n",
      "2017-05-17 23:34:22.624995 Validation loss: 1.3, accuracy: 0.52\n",
      "Epoch  9, CIFAR-10 Batch 2:  Training   loss: 0.94, accuracy: 0.65\n",
      "2017-05-17 23:34:24.836318 Validation loss: 1.3, accuracy: 0.53\n",
      "Epoch  9, CIFAR-10 Batch 3:  Training   loss: 0.87, accuracy: 0.65\n",
      "2017-05-17 23:34:27.073569 Validation loss: 1.2, accuracy: 0.55\n",
      "Epoch  9, CIFAR-10 Batch 4:  Training   loss: 0.9, accuracy: 0.7\n",
      "2017-05-17 23:34:29.280863 Validation loss: 1.3, accuracy: 0.54\n",
      "Epoch  9, CIFAR-10 Batch 5:  Training   loss: 1.0, accuracy: 0.68\n",
      "2017-05-17 23:34:31.507887 Validation loss: 1.2, accuracy: 0.56\n",
      "Epoch 10, CIFAR-10 Batch 1:  Training   loss: 1.0, accuracy: 0.62\n",
      "2017-05-17 23:34:33.738916 Validation loss: 1.2, accuracy: 0.55\n",
      "Epoch 10, CIFAR-10 Batch 2:  Training   loss: 0.92, accuracy: 0.57\n",
      "2017-05-17 23:34:35.949034 Validation loss: 1.3, accuracy: 0.53\n",
      "Epoch 10, CIFAR-10 Batch 3:  Training   loss: 0.81, accuracy: 0.7\n",
      "2017-05-17 23:34:38.154233 Validation loss: 1.2, accuracy: 0.55\n",
      "Epoch 10, CIFAR-10 Batch 4:  Training   loss: 0.94, accuracy: 0.7\n",
      "2017-05-17 23:34:40.365636 Validation loss: 1.3, accuracy: 0.54\n",
      "Epoch 10, CIFAR-10 Batch 5:  Training   loss: 0.95, accuracy: 0.73\n",
      "2017-05-17 23:34:42.598095 Validation loss: 1.2, accuracy: 0.57\n",
      "Epoch 11, CIFAR-10 Batch 1:  Training   loss: 0.96, accuracy: 0.62\n",
      "2017-05-17 23:34:44.830107 Validation loss: 1.2, accuracy: 0.56\n",
      "Epoch 11, CIFAR-10 Batch 2:  Training   loss: 0.86, accuracy: 0.73\n",
      "2017-05-17 23:34:47.069788 Validation loss: 1.2, accuracy: 0.56\n",
      "Epoch 11, CIFAR-10 Batch 3:  Training   loss: 0.77, accuracy: 0.68\n",
      "2017-05-17 23:34:49.313067 Validation loss: 1.2, accuracy: 0.55\n",
      "Epoch 11, CIFAR-10 Batch 4:  Training   loss: 0.78, accuracy: 0.75\n",
      "2017-05-17 23:34:51.533263 Validation loss: 1.2, accuracy: 0.57\n",
      "Epoch 11, CIFAR-10 Batch 5:  Training   loss: 0.95, accuracy: 0.75\n",
      "2017-05-17 23:34:53.758212 Validation loss: 1.2, accuracy: 0.56\n",
      "Epoch 12, CIFAR-10 Batch 1:  Training   loss: 0.87, accuracy: 0.62\n",
      "2017-05-17 23:34:55.968522 Validation loss: 1.2, accuracy: 0.58\n",
      "Epoch 12, CIFAR-10 Batch 2:  Training   loss: 0.78, accuracy: 0.73\n",
      "2017-05-17 23:34:58.176127 Validation loss: 1.2, accuracy: 0.56\n",
      "Epoch 12, CIFAR-10 Batch 3:  Training   loss: 0.68, accuracy: 0.68\n",
      "2017-05-17 23:35:00.407312 Validation loss: 1.2, accuracy: 0.57\n",
      "Epoch 12, CIFAR-10 Batch 4:  Training   loss: 0.72, accuracy: 0.77\n",
      "2017-05-17 23:35:02.670340 Validation loss: 1.2, accuracy: 0.58\n",
      "Epoch 12, CIFAR-10 Batch 5:  Training   loss: 0.84, accuracy: 0.75\n",
      "2017-05-17 23:35:04.886608 Validation loss: 1.2, accuracy: 0.57\n",
      "Epoch 13, CIFAR-10 Batch 1:  Training   loss: 0.81, accuracy: 0.68\n",
      "2017-05-17 23:35:07.108730 Validation loss: 1.2, accuracy: 0.59\n",
      "Epoch 13, CIFAR-10 Batch 2:  Training   loss: 0.74, accuracy: 0.73\n",
      "2017-05-17 23:35:09.318744 Validation loss: 1.2, accuracy: 0.58\n",
      "Epoch 13, CIFAR-10 Batch 3:  Training   loss: 0.63, accuracy: 0.75\n",
      "2017-05-17 23:35:11.533609 Validation loss: 1.2, accuracy: 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, CIFAR-10 Batch 4:  Training   loss: 0.74, accuracy: 0.83\n",
      "2017-05-17 23:35:13.745896 Validation loss: 1.2, accuracy: 0.58\n",
      "Epoch 13, CIFAR-10 Batch 5:  Training   loss: 0.79, accuracy: 0.75\n",
      "2017-05-17 23:35:15.957073 Validation loss: 1.2, accuracy: 0.58\n",
      "Epoch 14, CIFAR-10 Batch 1:  Training   loss: 0.76, accuracy: 0.75\n",
      "2017-05-17 23:35:18.163079 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 14, CIFAR-10 Batch 2:  Training   loss: 0.73, accuracy: 0.73\n",
      "2017-05-17 23:35:20.377490 Validation loss: 1.2, accuracy: 0.58\n",
      "Epoch 14, CIFAR-10 Batch 3:  Training   loss: 0.6, accuracy: 0.75\n",
      "2017-05-17 23:35:22.615213 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 14, CIFAR-10 Batch 4:  Training   loss: 0.67, accuracy: 0.8\n",
      "2017-05-17 23:35:24.830156 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 14, CIFAR-10 Batch 5:  Training   loss: 0.71, accuracy: 0.75\n",
      "2017-05-17 23:35:27.050952 Validation loss: 1.1, accuracy: 0.59\n",
      "Epoch 15, CIFAR-10 Batch 1:  Training   loss: 0.7, accuracy: 0.78\n",
      "2017-05-17 23:35:29.273099 Validation loss: 1.1, accuracy: 0.59\n",
      "Epoch 15, CIFAR-10 Batch 2:  Training   loss: 0.67, accuracy: 0.78\n",
      "2017-05-17 23:35:31.476355 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 15, CIFAR-10 Batch 3:  Training   loss: 0.65, accuracy: 0.7\n",
      "2017-05-17 23:35:33.687263 Validation loss: 1.2, accuracy: 0.58\n",
      "Epoch 15, CIFAR-10 Batch 4:  Training   loss: 0.67, accuracy: 0.83\n",
      "2017-05-17 23:35:35.898574 Validation loss: 1.1, accuracy: 0.59\n",
      "Epoch 15, CIFAR-10 Batch 5:  Training   loss: 0.65, accuracy: 0.73\n",
      "2017-05-17 23:35:38.141007 Validation loss: 1.1, accuracy: 0.59\n",
      "Epoch 16, CIFAR-10 Batch 1:  Training   loss: 0.7, accuracy: 0.75\n",
      "2017-05-17 23:35:40.354320 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 16, CIFAR-10 Batch 2:  Training   loss: 0.61, accuracy: 0.77\n",
      "2017-05-17 23:35:42.563508 Validation loss: 1.2, accuracy: 0.6\n",
      "Epoch 16, CIFAR-10 Batch 3:  Training   loss: 0.63, accuracy: 0.73\n",
      "2017-05-17 23:35:44.777294 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 16, CIFAR-10 Batch 4:  Training   loss: 0.7, accuracy: 0.88\n",
      "2017-05-17 23:35:47.012174 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 16, CIFAR-10 Batch 5:  Training   loss: 0.66, accuracy: 0.77\n",
      "2017-05-17 23:35:49.242382 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 17, CIFAR-10 Batch 1:  Training   loss: 0.71, accuracy: 0.7\n",
      "2017-05-17 23:35:51.485426 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 17, CIFAR-10 Batch 2:  Training   loss: 0.59, accuracy: 0.75\n",
      "2017-05-17 23:35:53.708807 Validation loss: 1.2, accuracy: 0.6\n",
      "Epoch 17, CIFAR-10 Batch 3:  Training   loss: 0.55, accuracy: 0.77\n",
      "2017-05-17 23:35:55.918121 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 17, CIFAR-10 Batch 4:  Training   loss: 0.65, accuracy: 0.8\n",
      "2017-05-17 23:35:58.144529 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 17, CIFAR-10 Batch 5:  Training   loss: 0.63, accuracy: 0.77\n",
      "2017-05-17 23:36:00.364820 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 18, CIFAR-10 Batch 1:  Training   loss: 0.68, accuracy: 0.73\n",
      "2017-05-17 23:36:02.576024 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 18, CIFAR-10 Batch 2:  Training   loss: 0.53, accuracy: 0.78\n",
      "2017-05-17 23:36:04.798981 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 18, CIFAR-10 Batch 3:  Training   loss: 0.52, accuracy: 0.78\n",
      "2017-05-17 23:36:07.020576 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 18, CIFAR-10 Batch 4:  Training   loss: 0.49, accuracy: 0.88\n",
      "2017-05-17 23:36:09.265416 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 18, CIFAR-10 Batch 5:  Training   loss: 0.57, accuracy: 0.83\n",
      "2017-05-17 23:36:11.478012 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 19, CIFAR-10 Batch 1:  Training   loss: 0.58, accuracy: 0.8\n",
      "2017-05-17 23:36:13.715392 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 19, CIFAR-10 Batch 2:  Training   loss: 0.51, accuracy: 0.77\n",
      "2017-05-17 23:36:15.943296 Validation loss: 1.1, accuracy: 0.6\n",
      "Epoch 19, CIFAR-10 Batch 3:  Training   loss: 0.56, accuracy: 0.75\n",
      "2017-05-17 23:36:18.182985 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 19, CIFAR-10 Batch 4:  Training   loss: 0.53, accuracy: 0.88\n",
      "2017-05-17 23:36:20.392214 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 19, CIFAR-10 Batch 5:  Training   loss: 0.52, accuracy: 0.83\n",
      "2017-05-17 23:36:22.604497 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 20, CIFAR-10 Batch 1:  Training   loss: 0.59, accuracy: 0.75\n",
      "2017-05-17 23:36:24.816070 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 20, CIFAR-10 Batch 2:  Training   loss: 0.5, accuracy: 0.77\n",
      "2017-05-17 23:36:27.045950 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 20, CIFAR-10 Batch 3:  Training   loss: 0.49, accuracy: 0.85\n",
      "2017-05-17 23:36:29.258345 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 20, CIFAR-10 Batch 4:  Training   loss: 0.54, accuracy: 0.93\n",
      "2017-05-17 23:36:31.488155 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 20, CIFAR-10 Batch 5:  Training   loss: 0.5, accuracy: 0.83\n",
      "2017-05-17 23:36:33.678361 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 21, CIFAR-10 Batch 1:  Training   loss: 0.54, accuracy: 0.85\n",
      "2017-05-17 23:36:35.922353 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 21, CIFAR-10 Batch 2:  Training   loss: 0.49, accuracy: 0.82\n",
      "2017-05-17 23:36:38.165880 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 21, CIFAR-10 Batch 3:  Training   loss: 0.42, accuracy: 0.9\n",
      "2017-05-17 23:36:40.443287 Validation loss: 1.1, accuracy: 0.61\n",
      "Epoch 21, CIFAR-10 Batch 4:  Training   loss: 0.48, accuracy: 0.93\n",
      "2017-05-17 23:36:42.722213 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 21, CIFAR-10 Batch 5:  Training   loss: 0.5, accuracy: 0.83\n",
      "2017-05-17 23:36:45.012070 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 22, CIFAR-10 Batch 1:  Training   loss: 0.52, accuracy: 0.85\n",
      "2017-05-17 23:36:47.287763 Validation loss: 1.0, accuracy: 0.63\n",
      "Epoch 22, CIFAR-10 Batch 2:  Training   loss: 0.48, accuracy: 0.88\n",
      "2017-05-17 23:36:49.565415 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 22, CIFAR-10 Batch 3:  Training   loss: 0.43, accuracy: 0.88\n",
      "2017-05-17 23:36:51.856207 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 22, CIFAR-10 Batch 4:  Training   loss: 0.47, accuracy: 0.88\n",
      "2017-05-17 23:36:54.142023 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 22, CIFAR-10 Batch 5:  Training   loss: 0.45, accuracy: 0.83\n",
      "2017-05-17 23:36:56.451662 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 23, CIFAR-10 Batch 1:  Training   loss: 0.53, accuracy: 0.82\n",
      "2017-05-17 23:36:58.816828 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 23, CIFAR-10 Batch 2:  Training   loss: 0.43, accuracy: 0.85\n",
      "2017-05-17 23:37:01.109351 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 23, CIFAR-10 Batch 3:  Training   loss: 0.39, accuracy: 0.9\n",
      "2017-05-17 23:37:03.405964 Validation loss: 1.1, accuracy: 0.62\n",
      "Epoch 23, CIFAR-10 Batch 4:  Training   loss: 0.46, accuracy: 0.9\n",
      "2017-05-17 23:37:05.689558 Validation loss: 1.0, accuracy: 0.63\n",
      "Epoch 23, CIFAR-10 Batch 5:  Training   loss: 0.42, accuracy: 0.88\n",
      "2017-05-17 23:37:08.035675 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 24, CIFAR-10 Batch 1:  Training   loss: 0.49, accuracy: 0.85\n",
      "2017-05-17 23:37:10.325066 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 24, CIFAR-10 Batch 2:  Training   loss: 0.42, accuracy: 0.88\n",
      "2017-05-17 23:37:12.615160 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 24, CIFAR-10 Batch 3:  Training   loss: 0.42, accuracy: 0.85\n",
      "2017-05-17 23:37:14.936229 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 24, CIFAR-10 Batch 4:  Training   loss: 0.44, accuracy: 0.9\n",
      "2017-05-17 23:37:17.239274 Validation loss: 1.0, accuracy: 0.63\n",
      "Epoch 24, CIFAR-10 Batch 5:  Training   loss: 0.4, accuracy: 0.92\n",
      "2017-05-17 23:37:19.582807 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 25, CIFAR-10 Batch 1:  Training   loss: 0.48, accuracy: 0.77\n",
      "2017-05-17 23:37:21.911273 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 25, CIFAR-10 Batch 2:  Training   loss: 0.42, accuracy: 0.85\n",
      "2017-05-17 23:37:24.251101 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 25, CIFAR-10 Batch 3:  Training   loss: 0.38, accuracy: 0.9\n",
      "2017-05-17 23:37:26.589745 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 25, CIFAR-10 Batch 4:  Training   loss: 0.41, accuracy: 0.93\n",
      "2017-05-17 23:37:28.959610 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 25, CIFAR-10 Batch 5:  Training   loss: 0.37, accuracy: 0.95\n",
      "2017-05-17 23:37:31.282067 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 26, CIFAR-10 Batch 1:  Training   loss: 0.45, accuracy: 0.85\n",
      "2017-05-17 23:37:33.583489 Validation loss: 1.0, accuracy: 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, CIFAR-10 Batch 2:  Training   loss: 0.42, accuracy: 0.93\n",
      "2017-05-17 23:37:35.940001 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 26, CIFAR-10 Batch 3:  Training   loss: 0.34, accuracy: 0.9\n",
      "2017-05-17 23:37:38.313072 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 26, CIFAR-10 Batch 4:  Training   loss: 0.43, accuracy: 0.93\n",
      "2017-05-17 23:37:40.629748 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 26, CIFAR-10 Batch 5:  Training   loss: 0.39, accuracy: 0.95\n",
      "2017-05-17 23:37:42.963261 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 27, CIFAR-10 Batch 1:  Training   loss: 0.5, accuracy: 0.9\n",
      "2017-05-17 23:37:45.286463 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 27, CIFAR-10 Batch 2:  Training   loss: 0.4, accuracy: 0.83\n",
      "2017-05-17 23:37:47.609978 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 27, CIFAR-10 Batch 3:  Training   loss: 0.36, accuracy: 0.9\n",
      "2017-05-17 23:37:49.963095 Validation loss: 1.1, accuracy: 0.63\n",
      "Epoch 27, CIFAR-10 Batch 4:  Training   loss: 0.38, accuracy: 0.93\n",
      "2017-05-17 23:37:52.302606 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 27, CIFAR-10 Batch 5:  Training   loss: 0.38, accuracy: 0.95\n",
      "2017-05-17 23:37:54.670388 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 28, CIFAR-10 Batch 1:  Training   loss: 0.46, accuracy: 0.85\n",
      "2017-05-17 23:37:57.065365 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 28, CIFAR-10 Batch 2:  Training   loss: 0.38, accuracy: 0.9\n",
      "2017-05-17 23:37:59.445893 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 28, CIFAR-10 Batch 3:  Training   loss: 0.31, accuracy: 0.93\n",
      "2017-05-17 23:38:01.768893 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 28, CIFAR-10 Batch 4:  Training   loss: 0.35, accuracy: 0.93\n",
      "2017-05-17 23:38:04.143530 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 28, CIFAR-10 Batch 5:  Training   loss: 0.36, accuracy: 0.95\n",
      "2017-05-17 23:38:06.454370 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 29, CIFAR-10 Batch 1:  Training   loss: 0.49, accuracy: 0.9\n",
      "2017-05-17 23:38:08.765464 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 29, CIFAR-10 Batch 2:  Training   loss: 0.38, accuracy: 0.88\n",
      "2017-05-17 23:38:11.145492 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 29, CIFAR-10 Batch 3:  Training   loss: 0.33, accuracy: 0.93\n",
      "2017-05-17 23:38:13.438758 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 29, CIFAR-10 Batch 4:  Training   loss: 0.38, accuracy: 0.9\n",
      "2017-05-17 23:38:15.786249 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 29, CIFAR-10 Batch 5:  Training   loss: 0.37, accuracy: 0.93\n",
      "2017-05-17 23:38:18.194138 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 30, CIFAR-10 Batch 1:  Training   loss: 0.43, accuracy: 0.88\n",
      "2017-05-17 23:38:20.551081 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 30, CIFAR-10 Batch 2:  Training   loss: 0.32, accuracy: 0.93\n",
      "2017-05-17 23:38:22.887603 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 30, CIFAR-10 Batch 3:  Training   loss: 0.33, accuracy: 0.95\n",
      "2017-05-17 23:38:25.300492 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 30, CIFAR-10 Batch 4:  Training   loss: 0.31, accuracy: 0.93\n",
      "2017-05-17 23:38:27.682415 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 30, CIFAR-10 Batch 5:  Training   loss: 0.34, accuracy: 0.9\n",
      "2017-05-17 23:38:30.005581 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 31, CIFAR-10 Batch 1:  Training   loss: 0.41, accuracy: 0.9\n",
      "2017-05-17 23:38:32.412986 Validation loss: 0.99, accuracy: 0.65\n",
      "Epoch 31, CIFAR-10 Batch 2:  Training   loss: 0.34, accuracy: 0.9\n",
      "2017-05-17 23:38:34.728386 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 31, CIFAR-10 Batch 3:  Training   loss: 0.32, accuracy: 0.88\n",
      "2017-05-17 23:38:37.136502 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 31, CIFAR-10 Batch 4:  Training   loss: 0.31, accuracy: 0.93\n",
      "2017-05-17 23:38:39.444221 Validation loss: 0.99, accuracy: 0.65\n",
      "Epoch 31, CIFAR-10 Batch 5:  Training   loss: 0.33, accuracy: 0.9\n",
      "2017-05-17 23:38:41.796368 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 32, CIFAR-10 Batch 1:  Training   loss: 0.42, accuracy: 0.9\n",
      "2017-05-17 23:38:44.140616 Validation loss: 1.0, accuracy: 0.66\n",
      "Epoch 32, CIFAR-10 Batch 2:  Training   loss: 0.35, accuracy: 0.9\n",
      "2017-05-17 23:38:46.521548 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 32, CIFAR-10 Batch 3:  Training   loss: 0.25, accuracy: 0.93\n",
      "2017-05-17 23:38:48.850085 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 32, CIFAR-10 Batch 4:  Training   loss: 0.37, accuracy: 0.93\n",
      "2017-05-17 23:38:51.186188 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 32, CIFAR-10 Batch 5:  Training   loss: 0.28, accuracy: 0.98\n",
      "2017-05-17 23:38:53.519670 Validation loss: 0.99, accuracy: 0.65\n",
      "Epoch 33, CIFAR-10 Batch 1:  Training   loss: 0.39, accuracy: 0.88\n",
      "2017-05-17 23:38:55.861943 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 33, CIFAR-10 Batch 2:  Training   loss: 0.33, accuracy: 0.88\n",
      "2017-05-17 23:38:58.241196 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 33, CIFAR-10 Batch 3:  Training   loss: 0.26, accuracy: 0.93\n",
      "2017-05-17 23:39:00.607371 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 33, CIFAR-10 Batch 4:  Training   loss: 0.31, accuracy: 0.93\n",
      "2017-05-17 23:39:02.972288 Validation loss: 0.97, accuracy: 0.66\n",
      "Epoch 33, CIFAR-10 Batch 5:  Training   loss: 0.26, accuracy: 0.98\n",
      "2017-05-17 23:39:05.357001 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 34, CIFAR-10 Batch 1:  Training   loss: 0.35, accuracy: 0.93\n",
      "2017-05-17 23:39:07.731261 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 34, CIFAR-10 Batch 2:  Training   loss: 0.33, accuracy: 0.9\n",
      "2017-05-17 23:39:10.079996 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 34, CIFAR-10 Batch 3:  Training   loss: 0.3, accuracy: 0.88\n",
      "2017-05-17 23:39:12.483865 Validation loss: 1.0, accuracy: 0.64\n",
      "Epoch 34, CIFAR-10 Batch 4:  Training   loss: 0.28, accuracy: 0.93\n",
      "2017-05-17 23:39:14.844209 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 34, CIFAR-10 Batch 5:  Training   loss: 0.29, accuracy: 0.95\n",
      "2017-05-17 23:39:17.221601 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 35, CIFAR-10 Batch 1:  Training   loss: 0.36, accuracy: 0.9\n",
      "2017-05-17 23:39:19.566299 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 35, CIFAR-10 Batch 2:  Training   loss: 0.29, accuracy: 0.9\n",
      "2017-05-17 23:39:21.913644 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 35, CIFAR-10 Batch 3:  Training   loss: 0.26, accuracy: 0.95\n",
      "2017-05-17 23:39:24.327891 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 35, CIFAR-10 Batch 4:  Training   loss: 0.32, accuracy: 0.9\n",
      "2017-05-17 23:39:26.730832 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 35, CIFAR-10 Batch 5:  Training   loss: 0.23, accuracy: 0.98\n",
      "2017-05-17 23:39:29.062942 Validation loss: 0.99, accuracy: 0.65\n",
      "Epoch 36, CIFAR-10 Batch 1:  Training   loss: 0.35, accuracy: 0.93\n",
      "2017-05-17 23:39:31.421734 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 36, CIFAR-10 Batch 2:  Training   loss: 0.33, accuracy: 0.9\n",
      "2017-05-17 23:39:33.753729 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 36, CIFAR-10 Batch 3:  Training   loss: 0.28, accuracy: 0.9\n",
      "2017-05-17 23:39:36.097234 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 36, CIFAR-10 Batch 4:  Training   loss: 0.3, accuracy: 0.95\n",
      "2017-05-17 23:39:38.453380 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 36, CIFAR-10 Batch 5:  Training   loss: 0.23, accuracy: 0.98\n",
      "2017-05-17 23:39:40.864411 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 37, CIFAR-10 Batch 1:  Training   loss: 0.32, accuracy: 0.9\n",
      "2017-05-17 23:39:43.244811 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 37, CIFAR-10 Batch 2:  Training   loss: 0.29, accuracy: 0.88\n",
      "2017-05-17 23:39:45.583494 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 37, CIFAR-10 Batch 3:  Training   loss: 0.25, accuracy: 0.95\n",
      "2017-05-17 23:39:47.939063 Validation loss: 0.99, accuracy: 0.65\n",
      "Epoch 37, CIFAR-10 Batch 4:  Training   loss: 0.28, accuracy: 0.93\n",
      "2017-05-17 23:39:50.277091 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 37, CIFAR-10 Batch 5:  Training   loss: 0.23, accuracy: 0.98\n",
      "2017-05-17 23:39:52.669139 Validation loss: 1.0, accuracy: 0.65\n",
      "Epoch 38, CIFAR-10 Batch 1:  Training   loss: 0.27, accuracy: 0.9\n",
      "2017-05-17 23:39:55.026217 Validation loss: 0.97, accuracy: 0.66\n",
      "Epoch 38, CIFAR-10 Batch 2:  Training   loss: 0.3, accuracy: 0.9\n",
      "2017-05-17 23:39:57.389382 Validation loss: 0.97, accuracy: 0.66\n",
      "Epoch 38, CIFAR-10 Batch 3:  Training   loss: 0.24, accuracy: 0.95\n",
      "2017-05-17 23:39:59.737364 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 38, CIFAR-10 Batch 4:  Training   loss: 0.31, accuracy: 0.93\n",
      "2017-05-17 23:40:02.122091 Validation loss: 0.95, accuracy: 0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, CIFAR-10 Batch 5:  Training   loss: 0.26, accuracy: 0.98\n",
      "2017-05-17 23:40:04.500371 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 39, CIFAR-10 Batch 1:  Training   loss: 0.27, accuracy: 0.93\n",
      "2017-05-17 23:40:06.869120 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 39, CIFAR-10 Batch 2:  Training   loss: 0.31, accuracy: 0.88\n",
      "2017-05-17 23:40:09.273697 Validation loss: 1.0, accuracy: 0.66\n",
      "Epoch 39, CIFAR-10 Batch 3:  Training   loss: 0.24, accuracy: 1.0\n",
      "2017-05-17 23:40:11.619422 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 39, CIFAR-10 Batch 4:  Training   loss: 0.26, accuracy: 0.95\n",
      "2017-05-17 23:40:13.999285 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 39, CIFAR-10 Batch 5:  Training   loss: 0.24, accuracy: 0.98\n",
      "2017-05-17 23:40:16.388767 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 40, CIFAR-10 Batch 1:  Training   loss: 0.31, accuracy: 0.9\n",
      "2017-05-17 23:40:18.769586 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 40, CIFAR-10 Batch 2:  Training   loss: 0.28, accuracy: 0.9\n",
      "2017-05-17 23:40:21.111132 Validation loss: 0.95, accuracy: 0.66\n",
      "Epoch 40, CIFAR-10 Batch 3:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:40:23.512219 Validation loss: 0.97, accuracy: 0.66\n",
      "Epoch 40, CIFAR-10 Batch 4:  Training   loss: 0.3, accuracy: 0.93\n",
      "2017-05-17 23:40:25.921112 Validation loss: 0.95, accuracy: 0.67\n",
      "Epoch 40, CIFAR-10 Batch 5:  Training   loss: 0.24, accuracy: 0.98\n",
      "2017-05-17 23:40:28.288411 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 41, CIFAR-10 Batch 1:  Training   loss: 0.26, accuracy: 0.95\n",
      "2017-05-17 23:40:30.591203 Validation loss: 0.95, accuracy: 0.67\n",
      "Epoch 41, CIFAR-10 Batch 2:  Training   loss: 0.26, accuracy: 0.95\n",
      "2017-05-17 23:40:32.918471 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 41, CIFAR-10 Batch 3:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:40:35.247796 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 41, CIFAR-10 Batch 4:  Training   loss: 0.3, accuracy: 0.93\n",
      "2017-05-17 23:40:37.634411 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 41, CIFAR-10 Batch 5:  Training   loss: 0.19, accuracy: 0.98\n",
      "2017-05-17 23:40:40.009720 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 42, CIFAR-10 Batch 1:  Training   loss: 0.24, accuracy: 0.95\n",
      "2017-05-17 23:40:42.406876 Validation loss: 1.0, accuracy: 0.66\n",
      "Epoch 42, CIFAR-10 Batch 2:  Training   loss: 0.27, accuracy: 0.9\n",
      "2017-05-17 23:40:44.771789 Validation loss: 0.95, accuracy: 0.67\n",
      "Epoch 42, CIFAR-10 Batch 3:  Training   loss: 0.2, accuracy: 1.0\n",
      "2017-05-17 23:40:47.123741 Validation loss: 0.97, accuracy: 0.66\n",
      "Epoch 42, CIFAR-10 Batch 4:  Training   loss: 0.25, accuracy: 0.95\n",
      "2017-05-17 23:40:49.501172 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 42, CIFAR-10 Batch 5:  Training   loss: 0.21, accuracy: 1.0\n",
      "2017-05-17 23:40:51.841468 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 43, CIFAR-10 Batch 1:  Training   loss: 0.28, accuracy: 0.93\n",
      "2017-05-17 23:40:54.238378 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 43, CIFAR-10 Batch 2:  Training   loss: 0.26, accuracy: 0.93\n",
      "2017-05-17 23:40:56.598462 Validation loss: 0.99, accuracy: 0.67\n",
      "Epoch 43, CIFAR-10 Batch 3:  Training   loss: 0.23, accuracy: 0.95\n",
      "2017-05-17 23:40:58.983754 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 43, CIFAR-10 Batch 4:  Training   loss: 0.24, accuracy: 0.93\n",
      "2017-05-17 23:41:01.318957 Validation loss: 0.97, accuracy: 0.66\n",
      "Epoch 43, CIFAR-10 Batch 5:  Training   loss: 0.19, accuracy: 1.0\n",
      "2017-05-17 23:41:03.715019 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 44, CIFAR-10 Batch 1:  Training   loss: 0.27, accuracy: 0.93\n",
      "2017-05-17 23:41:06.087128 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 44, CIFAR-10 Batch 2:  Training   loss: 0.29, accuracy: 0.93\n",
      "2017-05-17 23:41:08.440002 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 44, CIFAR-10 Batch 3:  Training   loss: 0.22, accuracy: 0.93\n",
      "2017-05-17 23:41:10.818988 Validation loss: 1.0, accuracy: 0.66\n",
      "Epoch 44, CIFAR-10 Batch 4:  Training   loss: 0.3, accuracy: 0.93\n",
      "2017-05-17 23:41:13.200162 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 44, CIFAR-10 Batch 5:  Training   loss: 0.18, accuracy: 0.97\n",
      "2017-05-17 23:41:15.584868 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 45, CIFAR-10 Batch 1:  Training   loss: 0.22, accuracy: 0.93\n",
      "2017-05-17 23:41:17.979207 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 45, CIFAR-10 Batch 2:  Training   loss: 0.26, accuracy: 0.9\n",
      "2017-05-17 23:41:20.339647 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 45, CIFAR-10 Batch 3:  Training   loss: 0.23, accuracy: 0.93\n",
      "2017-05-17 23:41:22.696091 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 45, CIFAR-10 Batch 4:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:41:25.074947 Validation loss: 0.94, accuracy: 0.67\n",
      "Epoch 45, CIFAR-10 Batch 5:  Training   loss: 0.17, accuracy: 1.0\n",
      "2017-05-17 23:41:27.433378 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 46, CIFAR-10 Batch 1:  Training   loss: 0.25, accuracy: 0.95\n",
      "2017-05-17 23:41:29.799362 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 46, CIFAR-10 Batch 2:  Training   loss: 0.27, accuracy: 0.9\n",
      "2017-05-17 23:41:32.177257 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 46, CIFAR-10 Batch 3:  Training   loss: 0.2, accuracy: 0.95\n",
      "2017-05-17 23:41:34.537438 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 46, CIFAR-10 Batch 4:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:41:36.883013 Validation loss: 0.92, accuracy: 0.68\n",
      "Epoch 46, CIFAR-10 Batch 5:  Training   loss: 0.18, accuracy: 1.0\n",
      "2017-05-17 23:41:39.285537 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 47, CIFAR-10 Batch 1:  Training   loss: 0.26, accuracy: 0.93\n",
      "2017-05-17 23:41:41.701749 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 47, CIFAR-10 Batch 2:  Training   loss: 0.27, accuracy: 0.88\n",
      "2017-05-17 23:41:44.096590 Validation loss: 0.94, accuracy: 0.67\n",
      "Epoch 47, CIFAR-10 Batch 3:  Training   loss: 0.18, accuracy: 0.97\n",
      "2017-05-17 23:41:46.474137 Validation loss: 0.99, accuracy: 0.66\n",
      "Epoch 47, CIFAR-10 Batch 4:  Training   loss: 0.23, accuracy: 0.95\n",
      "2017-05-17 23:41:48.832630 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 47, CIFAR-10 Batch 5:  Training   loss: 0.22, accuracy: 0.98\n",
      "2017-05-17 23:41:51.238922 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 48, CIFAR-10 Batch 1:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:41:53.609091 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 48, CIFAR-10 Batch 2:  Training   loss: 0.26, accuracy: 0.88\n",
      "2017-05-17 23:41:55.967630 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 48, CIFAR-10 Batch 3:  Training   loss: 0.18, accuracy: 0.95\n",
      "2017-05-17 23:41:58.341494 Validation loss: 0.98, accuracy: 0.66\n",
      "Epoch 48, CIFAR-10 Batch 4:  Training   loss: 0.24, accuracy: 0.93\n",
      "2017-05-17 23:42:00.738318 Validation loss: 0.95, accuracy: 0.67\n",
      "Epoch 48, CIFAR-10 Batch 5:  Training   loss: 0.17, accuracy: 1.0\n",
      "2017-05-17 23:42:03.126619 Validation loss: 0.96, accuracy: 0.68\n",
      "Epoch 49, CIFAR-10 Batch 1:  Training   loss: 0.22, accuracy: 0.97\n",
      "2017-05-17 23:42:05.499019 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 49, CIFAR-10 Batch 2:  Training   loss: 0.29, accuracy: 0.9\n",
      "2017-05-17 23:42:07.887534 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 49, CIFAR-10 Batch 3:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:42:10.289176 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 49, CIFAR-10 Batch 4:  Training   loss: 0.22, accuracy: 0.93\n",
      "2017-05-17 23:42:12.676152 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 49, CIFAR-10 Batch 5:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:42:15.097806 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 50, CIFAR-10 Batch 1:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:42:17.537744 Validation loss: 0.97, accuracy: 0.66\n",
      "Epoch 50, CIFAR-10 Batch 2:  Training   loss: 0.3, accuracy: 0.88\n",
      "2017-05-17 23:42:19.941414 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 50, CIFAR-10 Batch 3:  Training   loss: 0.16, accuracy: 1.0\n",
      "2017-05-17 23:42:22.292135 Validation loss: 0.95, accuracy: 0.67\n",
      "Epoch 50, CIFAR-10 Batch 4:  Training   loss: 0.21, accuracy: 0.95\n",
      "2017-05-17 23:42:24.666985 Validation loss: 0.92, accuracy: 0.68\n",
      "Epoch 50, CIFAR-10 Batch 5:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:42:27.051499 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 51, CIFAR-10 Batch 1:  Training   loss: 0.19, accuracy: 0.97\n",
      "2017-05-17 23:42:29.420552 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 51, CIFAR-10 Batch 2:  Training   loss: 0.26, accuracy: 0.88\n",
      "2017-05-17 23:42:31.900272 Validation loss: 0.97, accuracy: 0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, CIFAR-10 Batch 3:  Training   loss: 0.16, accuracy: 1.0\n",
      "2017-05-17 23:42:34.261755 Validation loss: 0.93, accuracy: 0.67\n",
      "Epoch 51, CIFAR-10 Batch 4:  Training   loss: 0.18, accuracy: 0.97\n",
      "2017-05-17 23:42:36.627204 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 51, CIFAR-10 Batch 5:  Training   loss: 0.15, accuracy: 1.0\n",
      "2017-05-17 23:42:39.036376 Validation loss: 0.96, accuracy: 0.67\n",
      "Epoch 52, CIFAR-10 Batch 1:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:42:41.406792 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 52, CIFAR-10 Batch 2:  Training   loss: 0.25, accuracy: 0.93\n",
      "2017-05-17 23:42:43.811960 Validation loss: 0.94, accuracy: 0.67\n",
      "Epoch 52, CIFAR-10 Batch 3:  Training   loss: 0.18, accuracy: 0.97\n",
      "2017-05-17 23:42:46.166335 Validation loss: 0.97, accuracy: 0.67\n",
      "Epoch 52, CIFAR-10 Batch 4:  Training   loss: 0.18, accuracy: 1.0\n",
      "2017-05-17 23:42:48.571346 Validation loss: 0.92, accuracy: 0.68\n",
      "Epoch 52, CIFAR-10 Batch 5:  Training   loss: 0.15, accuracy: 1.0\n",
      "2017-05-17 23:42:50.985083 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 53, CIFAR-10 Batch 1:  Training   loss: 0.2, accuracy: 0.95\n",
      "2017-05-17 23:42:53.431320 Validation loss: 0.98, accuracy: 0.67\n",
      "Epoch 53, CIFAR-10 Batch 2:  Training   loss: 0.25, accuracy: 0.9\n",
      "2017-05-17 23:42:55.823521 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 53, CIFAR-10 Batch 3:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:42:58.223447 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 53, CIFAR-10 Batch 4:  Training   loss: 0.19, accuracy: 0.95\n",
      "2017-05-17 23:43:00.568100 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 53, CIFAR-10 Batch 5:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:43:02.917471 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 54, CIFAR-10 Batch 1:  Training   loss: 0.19, accuracy: 0.97\n",
      "2017-05-17 23:43:05.298799 Validation loss: 0.92, accuracy: 0.68\n",
      "Epoch 54, CIFAR-10 Batch 2:  Training   loss: 0.24, accuracy: 0.88\n",
      "2017-05-17 23:43:07.680239 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 54, CIFAR-10 Batch 3:  Training   loss: 0.16, accuracy: 1.0\n",
      "2017-05-17 23:43:10.061671 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 54, CIFAR-10 Batch 4:  Training   loss: 0.2, accuracy: 1.0\n",
      "2017-05-17 23:43:12.407482 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 54, CIFAR-10 Batch 5:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:43:14.807333 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 55, CIFAR-10 Batch 1:  Training   loss: 0.18, accuracy: 0.95\n",
      "2017-05-17 23:43:17.237777 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 55, CIFAR-10 Batch 2:  Training   loss: 0.22, accuracy: 0.9\n",
      "2017-05-17 23:43:19.654360 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 55, CIFAR-10 Batch 3:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:43:22.051541 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 55, CIFAR-10 Batch 4:  Training   loss: 0.21, accuracy: 0.98\n",
      "2017-05-17 23:43:24.438727 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 55, CIFAR-10 Batch 5:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:43:26.835613 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 56, CIFAR-10 Batch 1:  Training   loss: 0.16, accuracy: 0.95\n",
      "2017-05-17 23:43:29.243148 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 56, CIFAR-10 Batch 2:  Training   loss: 0.2, accuracy: 0.93\n",
      "2017-05-17 23:43:31.646060 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 56, CIFAR-10 Batch 3:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:43:33.991409 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 56, CIFAR-10 Batch 4:  Training   loss: 0.18, accuracy: 1.0\n",
      "2017-05-17 23:43:36.408011 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 56, CIFAR-10 Batch 5:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:43:38.806238 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 57, CIFAR-10 Batch 1:  Training   loss: 0.22, accuracy: 0.95\n",
      "2017-05-17 23:43:41.151802 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 57, CIFAR-10 Batch 2:  Training   loss: 0.23, accuracy: 0.93\n",
      "2017-05-17 23:43:43.480178 Validation loss: 0.95, accuracy: 0.67\n",
      "Epoch 57, CIFAR-10 Batch 3:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:43:45.860091 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 57, CIFAR-10 Batch 4:  Training   loss: 0.16, accuracy: 0.97\n",
      "2017-05-17 23:43:48.236223 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 57, CIFAR-10 Batch 5:  Training   loss: 0.15, accuracy: 1.0\n",
      "2017-05-17 23:43:50.634032 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 58, CIFAR-10 Batch 1:  Training   loss: 0.18, accuracy: 0.95\n",
      "2017-05-17 23:43:53.011578 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 58, CIFAR-10 Batch 2:  Training   loss: 0.18, accuracy: 0.95\n",
      "2017-05-17 23:43:55.412336 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 58, CIFAR-10 Batch 3:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:43:57.835346 Validation loss: 0.95, accuracy: 0.67\n",
      "Epoch 58, CIFAR-10 Batch 4:  Training   loss: 0.16, accuracy: 1.0\n",
      "2017-05-17 23:44:00.181241 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 58, CIFAR-10 Batch 5:  Training   loss: 0.12, accuracy: 1.0\n",
      "2017-05-17 23:44:02.528004 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 59, CIFAR-10 Batch 1:  Training   loss: 0.16, accuracy: 0.97\n",
      "2017-05-17 23:44:04.916269 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 59, CIFAR-10 Batch 2:  Training   loss: 0.19, accuracy: 0.95\n",
      "2017-05-17 23:44:07.317267 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 59, CIFAR-10 Batch 3:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:44:09.707939 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 59, CIFAR-10 Batch 4:  Training   loss: 0.17, accuracy: 0.97\n",
      "2017-05-17 23:44:12.079902 Validation loss: 0.89, accuracy: 0.69\n",
      "Epoch 59, CIFAR-10 Batch 5:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:44:14.485321 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 60, CIFAR-10 Batch 1:  Training   loss: 0.18, accuracy: 0.97\n",
      "2017-05-17 23:44:16.828804 Validation loss: 0.92, accuracy: 0.68\n",
      "Epoch 60, CIFAR-10 Batch 2:  Training   loss: 0.2, accuracy: 0.95\n",
      "2017-05-17 23:44:19.226127 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 60, CIFAR-10 Batch 3:  Training   loss: 0.088, accuracy: 1.0\n",
      "2017-05-17 23:44:21.592147 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 60, CIFAR-10 Batch 4:  Training   loss: 0.16, accuracy: 1.0\n",
      "2017-05-17 23:44:23.974209 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 60, CIFAR-10 Batch 5:  Training   loss: 0.099, accuracy: 1.0\n",
      "2017-05-17 23:44:26.377721 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 61, CIFAR-10 Batch 1:  Training   loss: 0.18, accuracy: 0.95\n",
      "2017-05-17 23:44:28.772342 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 61, CIFAR-10 Batch 2:  Training   loss: 0.17, accuracy: 0.97\n",
      "2017-05-17 23:44:31.124851 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 61, CIFAR-10 Batch 3:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:44:33.587833 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 61, CIFAR-10 Batch 4:  Training   loss: 0.17, accuracy: 0.98\n",
      "2017-05-17 23:44:36.005228 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 61, CIFAR-10 Batch 5:  Training   loss: 0.12, accuracy: 1.0\n",
      "2017-05-17 23:44:38.355270 Validation loss: 0.95, accuracy: 0.69\n",
      "Epoch 62, CIFAR-10 Batch 1:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:44:40.710060 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 62, CIFAR-10 Batch 2:  Training   loss: 0.18, accuracy: 0.97\n",
      "2017-05-17 23:44:43.115626 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 62, CIFAR-10 Batch 3:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:44:45.455820 Validation loss: 0.96, accuracy: 0.69\n",
      "Epoch 62, CIFAR-10 Batch 4:  Training   loss: 0.16, accuracy: 1.0\n",
      "2017-05-17 23:44:47.841174 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 62, CIFAR-10 Batch 5:  Training   loss: 0.12, accuracy: 1.0\n",
      "2017-05-17 23:44:50.214080 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 63, CIFAR-10 Batch 1:  Training   loss: 0.17, accuracy: 0.95\n",
      "2017-05-17 23:44:52.605150 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 63, CIFAR-10 Batch 2:  Training   loss: 0.19, accuracy: 0.95\n",
      "2017-05-17 23:44:54.998123 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 63, CIFAR-10 Batch 3:  Training   loss: 0.097, accuracy: 1.0\n",
      "2017-05-17 23:44:57.375029 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 63, CIFAR-10 Batch 4:  Training   loss: 0.17, accuracy: 0.97\n",
      "2017-05-17 23:44:59.745634 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 63, CIFAR-10 Batch 5:  Training   loss: 0.16, accuracy: 0.98\n",
      "2017-05-17 23:45:02.129760 Validation loss: 0.91, accuracy: 0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, CIFAR-10 Batch 1:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:45:04.476419 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 64, CIFAR-10 Batch 2:  Training   loss: 0.18, accuracy: 0.95\n",
      "2017-05-17 23:45:06.855348 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 64, CIFAR-10 Batch 3:  Training   loss: 0.098, accuracy: 1.0\n",
      "2017-05-17 23:45:09.253803 Validation loss: 0.93, accuracy: 0.68\n",
      "Epoch 64, CIFAR-10 Batch 4:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:45:11.662616 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 64, CIFAR-10 Batch 5:  Training   loss: 0.088, accuracy: 1.0\n",
      "2017-05-17 23:45:14.040957 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 65, CIFAR-10 Batch 1:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:45:16.415826 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 65, CIFAR-10 Batch 2:  Training   loss: 0.17, accuracy: 0.97\n",
      "2017-05-17 23:45:18.807261 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 65, CIFAR-10 Batch 3:  Training   loss: 0.12, accuracy: 1.0\n",
      "2017-05-17 23:45:21.183417 Validation loss: 0.95, accuracy: 0.68\n",
      "Epoch 65, CIFAR-10 Batch 4:  Training   loss: 0.15, accuracy: 1.0\n",
      "2017-05-17 23:45:23.542604 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 65, CIFAR-10 Batch 5:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:45:25.896374 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 66, CIFAR-10 Batch 1:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:45:28.277176 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 66, CIFAR-10 Batch 2:  Training   loss: 0.15, accuracy: 0.98\n",
      "2017-05-17 23:45:30.648743 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 66, CIFAR-10 Batch 3:  Training   loss: 0.084, accuracy: 1.0\n",
      "2017-05-17 23:45:33.039224 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 66, CIFAR-10 Batch 4:  Training   loss: 0.14, accuracy: 1.0\n",
      "2017-05-17 23:45:35.402450 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 66, CIFAR-10 Batch 5:  Training   loss: 0.09, accuracy: 1.0\n",
      "2017-05-17 23:45:37.762469 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 67, CIFAR-10 Batch 1:  Training   loss: 0.12, accuracy: 0.97\n",
      "2017-05-17 23:45:40.126956 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 67, CIFAR-10 Batch 2:  Training   loss: 0.13, accuracy: 0.98\n",
      "2017-05-17 23:45:42.487465 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 67, CIFAR-10 Batch 3:  Training   loss: 0.085, accuracy: 1.0\n",
      "2017-05-17 23:45:44.909892 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 67, CIFAR-10 Batch 4:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:45:47.305397 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 67, CIFAR-10 Batch 5:  Training   loss: 0.098, accuracy: 1.0\n",
      "2017-05-17 23:45:49.671042 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 68, CIFAR-10 Batch 1:  Training   loss: 0.13, accuracy: 0.97\n",
      "2017-05-17 23:45:52.034856 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 68, CIFAR-10 Batch 2:  Training   loss: 0.15, accuracy: 0.97\n",
      "2017-05-17 23:45:54.410126 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 68, CIFAR-10 Batch 3:  Training   loss: 0.078, accuracy: 1.0\n",
      "2017-05-17 23:45:56.831269 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 68, CIFAR-10 Batch 4:  Training   loss: 0.13, accuracy: 0.97\n",
      "2017-05-17 23:45:59.197918 Validation loss: 0.88, accuracy: 0.7\n",
      "Epoch 68, CIFAR-10 Batch 5:  Training   loss: 0.1, accuracy: 1.0\n",
      "2017-05-17 23:46:01.541618 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 69, CIFAR-10 Batch 1:  Training   loss: 0.12, accuracy: 0.97\n",
      "2017-05-17 23:46:03.922012 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 69, CIFAR-10 Batch 2:  Training   loss: 0.15, accuracy: 1.0\n",
      "2017-05-17 23:46:06.349971 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 69, CIFAR-10 Batch 3:  Training   loss: 0.092, accuracy: 1.0\n",
      "2017-05-17 23:46:08.758048 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 69, CIFAR-10 Batch 4:  Training   loss: 0.13, accuracy: 0.97\n",
      "2017-05-17 23:46:11.145474 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 69, CIFAR-10 Batch 5:  Training   loss: 0.093, accuracy: 1.0\n",
      "2017-05-17 23:46:13.500100 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 70, CIFAR-10 Batch 1:  Training   loss: 0.1, accuracy: 1.0\n",
      "2017-05-17 23:46:15.877002 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 70, CIFAR-10 Batch 2:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:46:18.234057 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 70, CIFAR-10 Batch 3:  Training   loss: 0.065, accuracy: 1.0\n",
      "2017-05-17 23:46:20.702215 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 70, CIFAR-10 Batch 4:  Training   loss: 0.15, accuracy: 0.95\n",
      "2017-05-17 23:46:23.125513 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 70, CIFAR-10 Batch 5:  Training   loss: 0.098, accuracy: 0.95\n",
      "2017-05-17 23:46:25.488610 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 71, CIFAR-10 Batch 1:  Training   loss: 0.16, accuracy: 0.97\n",
      "2017-05-17 23:46:27.845713 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 71, CIFAR-10 Batch 2:  Training   loss: 0.16, accuracy: 0.97\n",
      "2017-05-17 23:46:30.219188 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 71, CIFAR-10 Batch 3:  Training   loss: 0.095, accuracy: 1.0\n",
      "2017-05-17 23:46:32.594802 Validation loss: 0.97, accuracy: 0.68\n",
      "Epoch 71, CIFAR-10 Batch 4:  Training   loss: 0.17, accuracy: 0.95\n",
      "2017-05-17 23:46:35.011751 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 71, CIFAR-10 Batch 5:  Training   loss: 0.082, accuracy: 1.0\n",
      "2017-05-17 23:46:37.399631 Validation loss: 0.88, accuracy: 0.7\n",
      "Epoch 72, CIFAR-10 Batch 1:  Training   loss: 0.15, accuracy: 0.97\n",
      "2017-05-17 23:46:39.744421 Validation loss: 0.94, accuracy: 0.69\n",
      "Epoch 72, CIFAR-10 Batch 2:  Training   loss: 0.16, accuracy: 0.97\n",
      "2017-05-17 23:46:42.094947 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 72, CIFAR-10 Batch 3:  Training   loss: 0.068, accuracy: 1.0\n",
      "2017-05-17 23:46:44.457461 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 72, CIFAR-10 Batch 4:  Training   loss: 0.12, accuracy: 1.0\n",
      "2017-05-17 23:46:46.840999 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 72, CIFAR-10 Batch 5:  Training   loss: 0.086, accuracy: 1.0\n",
      "2017-05-17 23:46:49.214961 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 73, CIFAR-10 Batch 1:  Training   loss: 0.13, accuracy: 0.97\n",
      "2017-05-17 23:46:51.599115 Validation loss: 0.94, accuracy: 0.69\n",
      "Epoch 73, CIFAR-10 Batch 2:  Training   loss: 0.17, accuracy: 0.95\n",
      "2017-05-17 23:46:53.972060 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 73, CIFAR-10 Batch 3:  Training   loss: 0.062, accuracy: 1.0\n",
      "2017-05-17 23:46:56.343901 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 73, CIFAR-10 Batch 4:  Training   loss: 0.15, accuracy: 0.97\n",
      "2017-05-17 23:46:58.674024 Validation loss: 0.95, accuracy: 0.69\n",
      "Epoch 73, CIFAR-10 Batch 5:  Training   loss: 0.089, accuracy: 1.0\n",
      "2017-05-17 23:47:01.050491 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 74, CIFAR-10 Batch 1:  Training   loss: 0.12, accuracy: 1.0\n",
      "2017-05-17 23:47:03.432368 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 74, CIFAR-10 Batch 2:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:47:05.817333 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 74, CIFAR-10 Batch 3:  Training   loss: 0.09, accuracy: 1.0\n",
      "2017-05-17 23:47:08.197772 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 74, CIFAR-10 Batch 4:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:47:10.581377 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 74, CIFAR-10 Batch 5:  Training   loss: 0.09, accuracy: 1.0\n",
      "2017-05-17 23:47:13.015179 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 75, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:47:15.432957 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 75, CIFAR-10 Batch 2:  Training   loss: 0.16, accuracy: 0.97\n",
      "2017-05-17 23:47:17.834823 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 75, CIFAR-10 Batch 3:  Training   loss: 0.075, accuracy: 1.0\n",
      "2017-05-17 23:47:20.186162 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 75, CIFAR-10 Batch 4:  Training   loss: 0.13, accuracy: 0.97\n",
      "2017-05-17 23:47:22.554393 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 75, CIFAR-10 Batch 5:  Training   loss: 0.082, accuracy: 1.0\n",
      "2017-05-17 23:47:24.921603 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 76, CIFAR-10 Batch 1:  Training   loss: 0.13, accuracy: 0.97\n",
      "2017-05-17 23:47:27.294678 Validation loss: 0.94, accuracy: 0.69\n",
      "Epoch 76, CIFAR-10 Batch 2:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:47:29.696990 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 76, CIFAR-10 Batch 3:  Training   loss: 0.094, accuracy: 1.0\n",
      "2017-05-17 23:47:32.120154 Validation loss: 0.93, accuracy: 0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, CIFAR-10 Batch 4:  Training   loss: 0.13, accuracy: 1.0\n",
      "2017-05-17 23:47:34.533343 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 76, CIFAR-10 Batch 5:  Training   loss: 0.083, accuracy: 1.0\n",
      "2017-05-17 23:47:36.912155 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 77, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:47:39.270423 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 77, CIFAR-10 Batch 2:  Training   loss: 0.15, accuracy: 0.95\n",
      "2017-05-17 23:47:41.680501 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 77, CIFAR-10 Batch 3:  Training   loss: 0.073, accuracy: 1.0\n",
      "2017-05-17 23:47:44.094117 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 77, CIFAR-10 Batch 4:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:47:46.502431 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 77, CIFAR-10 Batch 5:  Training   loss: 0.088, accuracy: 1.0\n",
      "2017-05-17 23:47:48.879192 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 78, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:47:51.203858 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 78, CIFAR-10 Batch 2:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:47:53.575954 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 78, CIFAR-10 Batch 3:  Training   loss: 0.065, accuracy: 1.0\n",
      "2017-05-17 23:47:55.955658 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 78, CIFAR-10 Batch 4:  Training   loss: 0.1, accuracy: 1.0\n",
      "2017-05-17 23:47:58.293391 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 78, CIFAR-10 Batch 5:  Training   loss: 0.067, accuracy: 1.0\n",
      "2017-05-17 23:48:00.640688 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 79, CIFAR-10 Batch 1:  Training   loss: 0.12, accuracy: 0.97\n",
      "2017-05-17 23:48:02.991751 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 79, CIFAR-10 Batch 2:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:48:05.382445 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 79, CIFAR-10 Batch 3:  Training   loss: 0.065, accuracy: 1.0\n",
      "2017-05-17 23:48:07.745269 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 79, CIFAR-10 Batch 4:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:48:10.107885 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 79, CIFAR-10 Batch 5:  Training   loss: 0.066, accuracy: 1.0\n",
      "2017-05-17 23:48:12.492987 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 80, CIFAR-10 Batch 1:  Training   loss: 0.14, accuracy: 0.97\n",
      "2017-05-17 23:48:14.908163 Validation loss: 0.9, accuracy: 0.69\n",
      "Epoch 80, CIFAR-10 Batch 2:  Training   loss: 0.16, accuracy: 0.95\n",
      "2017-05-17 23:48:17.309701 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 80, CIFAR-10 Batch 3:  Training   loss: 0.08, accuracy: 1.0\n",
      "2017-05-17 23:48:19.625437 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 80, CIFAR-10 Batch 4:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:48:21.982078 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 80, CIFAR-10 Batch 5:  Training   loss: 0.078, accuracy: 1.0\n",
      "2017-05-17 23:48:24.386120 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 81, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:48:26.765095 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 81, CIFAR-10 Batch 2:  Training   loss: 0.16, accuracy: 1.0\n",
      "2017-05-17 23:48:29.100285 Validation loss: 0.97, accuracy: 0.68\n",
      "Epoch 81, CIFAR-10 Batch 3:  Training   loss: 0.067, accuracy: 1.0\n",
      "2017-05-17 23:48:31.504529 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 81, CIFAR-10 Batch 4:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:48:33.863213 Validation loss: 0.94, accuracy: 0.68\n",
      "Epoch 81, CIFAR-10 Batch 5:  Training   loss: 0.087, accuracy: 1.0\n",
      "2017-05-17 23:48:36.188685 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 82, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:48:38.572347 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 82, CIFAR-10 Batch 2:  Training   loss: 0.12, accuracy: 0.97\n",
      "2017-05-17 23:48:40.937790 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 82, CIFAR-10 Batch 3:  Training   loss: 0.058, accuracy: 1.0\n",
      "2017-05-17 23:48:43.335816 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 82, CIFAR-10 Batch 4:  Training   loss: 0.089, accuracy: 1.0\n",
      "2017-05-17 23:48:45.712048 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 82, CIFAR-10 Batch 5:  Training   loss: 0.07, accuracy: 1.0\n",
      "2017-05-17 23:48:48.048585 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 83, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:48:50.440739 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 83, CIFAR-10 Batch 2:  Training   loss: 0.13, accuracy: 0.97\n",
      "2017-05-17 23:48:52.793897 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 83, CIFAR-10 Batch 3:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:48:55.180898 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 83, CIFAR-10 Batch 4:  Training   loss: 0.095, accuracy: 1.0\n",
      "2017-05-17 23:48:57.516826 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 83, CIFAR-10 Batch 5:  Training   loss: 0.054, accuracy: 1.0\n",
      "2017-05-17 23:48:59.918564 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 84, CIFAR-10 Batch 1:  Training   loss: 0.1, accuracy: 1.0\n",
      "2017-05-17 23:49:02.332651 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 84, CIFAR-10 Batch 2:  Training   loss: 0.15, accuracy: 0.95\n",
      "2017-05-17 23:49:04.714443 Validation loss: 0.94, accuracy: 0.69\n",
      "Epoch 84, CIFAR-10 Batch 3:  Training   loss: 0.065, accuracy: 1.0\n",
      "2017-05-17 23:49:07.138301 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 84, CIFAR-10 Batch 4:  Training   loss: 0.096, accuracy: 1.0\n",
      "2017-05-17 23:49:09.504042 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 84, CIFAR-10 Batch 5:  Training   loss: 0.073, accuracy: 1.0\n",
      "2017-05-17 23:49:11.864526 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 85, CIFAR-10 Batch 1:  Training   loss: 0.1, accuracy: 0.97\n",
      "2017-05-17 23:49:14.223376 Validation loss: 0.94, accuracy: 0.69\n",
      "Epoch 85, CIFAR-10 Batch 2:  Training   loss: 0.12, accuracy: 1.0\n",
      "2017-05-17 23:49:16.586410 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 85, CIFAR-10 Batch 3:  Training   loss: 0.078, accuracy: 0.97\n",
      "2017-05-17 23:49:18.958721 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 85, CIFAR-10 Batch 4:  Training   loss: 0.092, accuracy: 1.0\n",
      "2017-05-17 23:49:21.373328 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 85, CIFAR-10 Batch 5:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:49:23.740928 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 86, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:49:26.079831 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 86, CIFAR-10 Batch 2:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:49:28.407551 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 86, CIFAR-10 Batch 3:  Training   loss: 0.061, accuracy: 1.0\n",
      "2017-05-17 23:49:30.779140 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 86, CIFAR-10 Batch 4:  Training   loss: 0.09, accuracy: 1.0\n",
      "2017-05-17 23:49:33.106534 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 86, CIFAR-10 Batch 5:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:49:35.472934 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 87, CIFAR-10 Batch 1:  Training   loss: 0.1, accuracy: 0.95\n",
      "2017-05-17 23:49:37.825207 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 87, CIFAR-10 Batch 2:  Training   loss: 0.09, accuracy: 1.0\n",
      "2017-05-17 23:49:40.215785 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 87, CIFAR-10 Batch 3:  Training   loss: 0.045, accuracy: 1.0\n",
      "2017-05-17 23:49:42.590529 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 87, CIFAR-10 Batch 4:  Training   loss: 0.075, accuracy: 1.0\n",
      "2017-05-17 23:49:44.934250 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 87, CIFAR-10 Batch 5:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:49:47.352181 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 88, CIFAR-10 Batch 1:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:49:49.820635 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 88, CIFAR-10 Batch 2:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:49:52.218570 Validation loss: 0.97, accuracy: 0.69\n",
      "Epoch 88, CIFAR-10 Batch 3:  Training   loss: 0.063, accuracy: 1.0\n",
      "2017-05-17 23:49:54.584380 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 88, CIFAR-10 Batch 4:  Training   loss: 0.072, accuracy: 1.0\n",
      "2017-05-17 23:49:56.909995 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 88, CIFAR-10 Batch 5:  Training   loss: 0.049, accuracy: 1.0\n",
      "2017-05-17 23:49:59.266051 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 89, CIFAR-10 Batch 1:  Training   loss: 0.096, accuracy: 0.97\n",
      "2017-05-17 23:50:01.625042 Validation loss: 0.92, accuracy: 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, CIFAR-10 Batch 2:  Training   loss: 0.096, accuracy: 1.0\n",
      "2017-05-17 23:50:04.000241 Validation loss: 0.93, accuracy: 0.69\n",
      "Epoch 89, CIFAR-10 Batch 3:  Training   loss: 0.071, accuracy: 1.0\n",
      "2017-05-17 23:50:06.361025 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 89, CIFAR-10 Batch 4:  Training   loss: 0.062, accuracy: 1.0\n",
      "2017-05-17 23:50:08.721766 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 89, CIFAR-10 Batch 5:  Training   loss: 0.059, accuracy: 1.0\n",
      "2017-05-17 23:50:11.099942 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 90, CIFAR-10 Batch 1:  Training   loss: 0.1, accuracy: 1.0\n",
      "2017-05-17 23:50:13.433232 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 90, CIFAR-10 Batch 2:  Training   loss: 0.11, accuracy: 0.97\n",
      "2017-05-17 23:50:15.833657 Validation loss: 0.97, accuracy: 0.69\n",
      "Epoch 90, CIFAR-10 Batch 3:  Training   loss: 0.076, accuracy: 1.0\n",
      "2017-05-17 23:50:18.252333 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 90, CIFAR-10 Batch 4:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:50:20.637292 Validation loss: 0.89, accuracy: 0.7\n",
      "Epoch 90, CIFAR-10 Batch 5:  Training   loss: 0.062, accuracy: 1.0\n",
      "2017-05-17 23:50:22.972988 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 91, CIFAR-10 Batch 1:  Training   loss: 0.093, accuracy: 1.0\n",
      "2017-05-17 23:50:25.395299 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 91, CIFAR-10 Batch 2:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:50:27.784288 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 91, CIFAR-10 Batch 3:  Training   loss: 0.04, accuracy: 1.0\n",
      "2017-05-17 23:50:30.127665 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 91, CIFAR-10 Batch 4:  Training   loss: 0.062, accuracy: 1.0\n",
      "2017-05-17 23:50:32.487433 Validation loss: 0.88, accuracy: 0.7\n",
      "Epoch 91, CIFAR-10 Batch 5:  Training   loss: 0.073, accuracy: 1.0\n",
      "2017-05-17 23:50:34.864103 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 92, CIFAR-10 Batch 1:  Training   loss: 0.086, accuracy: 1.0\n",
      "2017-05-17 23:50:37.212110 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 92, CIFAR-10 Batch 2:  Training   loss: 0.097, accuracy: 1.0\n",
      "2017-05-17 23:50:39.571126 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 92, CIFAR-10 Batch 3:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:50:41.960232 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 92, CIFAR-10 Batch 4:  Training   loss: 0.095, accuracy: 1.0\n",
      "2017-05-17 23:50:44.351034 Validation loss: 0.95, accuracy: 0.69\n",
      "Epoch 92, CIFAR-10 Batch 5:  Training   loss: 0.065, accuracy: 1.0\n",
      "2017-05-17 23:50:46.748219 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 93, CIFAR-10 Batch 1:  Training   loss: 0.087, accuracy: 1.0\n",
      "2017-05-17 23:50:49.131961 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 93, CIFAR-10 Batch 2:  Training   loss: 0.095, accuracy: 1.0\n",
      "2017-05-17 23:50:51.483930 Validation loss: 0.94, accuracy: 0.7\n",
      "Epoch 93, CIFAR-10 Batch 3:  Training   loss: 0.058, accuracy: 1.0\n",
      "2017-05-17 23:50:53.840176 Validation loss: 0.94, accuracy: 0.7\n",
      "Epoch 93, CIFAR-10 Batch 4:  Training   loss: 0.077, accuracy: 1.0\n",
      "2017-05-17 23:50:56.211208 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 93, CIFAR-10 Batch 5:  Training   loss: 0.05, accuracy: 1.0\n",
      "2017-05-17 23:50:58.552954 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 94, CIFAR-10 Batch 1:  Training   loss: 0.091, accuracy: 1.0\n",
      "2017-05-17 23:51:00.944368 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 94, CIFAR-10 Batch 2:  Training   loss: 0.079, accuracy: 1.0\n",
      "2017-05-17 23:51:03.312114 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 94, CIFAR-10 Batch 3:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:51:05.649790 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 94, CIFAR-10 Batch 4:  Training   loss: 0.083, accuracy: 1.0\n",
      "2017-05-17 23:51:07.941851 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 94, CIFAR-10 Batch 5:  Training   loss: 0.049, accuracy: 1.0\n",
      "2017-05-17 23:51:10.355764 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 95, CIFAR-10 Batch 1:  Training   loss: 0.074, accuracy: 1.0\n",
      "2017-05-17 23:51:12.735152 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 95, CIFAR-10 Batch 2:  Training   loss: 0.11, accuracy: 1.0\n",
      "2017-05-17 23:51:15.142927 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 95, CIFAR-10 Batch 3:  Training   loss: 0.086, accuracy: 1.0\n",
      "2017-05-17 23:51:17.543319 Validation loss: 0.94, accuracy: 0.7\n",
      "Epoch 95, CIFAR-10 Batch 4:  Training   loss: 0.08, accuracy: 1.0\n",
      "2017-05-17 23:51:19.925346 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 95, CIFAR-10 Batch 5:  Training   loss: 0.039, accuracy: 1.0\n",
      "2017-05-17 23:51:22.286404 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 96, CIFAR-10 Batch 1:  Training   loss: 0.1, accuracy: 1.0\n",
      "2017-05-17 23:51:24.635321 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 96, CIFAR-10 Batch 2:  Training   loss: 0.095, accuracy: 1.0\n",
      "2017-05-17 23:51:26.990355 Validation loss: 0.95, accuracy: 0.69\n",
      "Epoch 96, CIFAR-10 Batch 3:  Training   loss: 0.068, accuracy: 1.0\n",
      "2017-05-17 23:51:29.359560 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 96, CIFAR-10 Batch 4:  Training   loss: 0.071, accuracy: 1.0\n",
      "2017-05-17 23:51:31.724302 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 96, CIFAR-10 Batch 5:  Training   loss: 0.039, accuracy: 1.0\n",
      "2017-05-17 23:51:34.125855 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 97, CIFAR-10 Batch 1:  Training   loss: 0.079, accuracy: 1.0\n",
      "2017-05-17 23:51:36.490493 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 97, CIFAR-10 Batch 2:  Training   loss: 0.083, accuracy: 1.0\n",
      "2017-05-17 23:51:38.873693 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 97, CIFAR-10 Batch 3:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:51:41.235377 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 97, CIFAR-10 Batch 4:  Training   loss: 0.087, accuracy: 1.0\n",
      "2017-05-17 23:51:43.615500 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 97, CIFAR-10 Batch 5:  Training   loss: 0.047, accuracy: 1.0\n",
      "2017-05-17 23:51:45.995010 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 98, CIFAR-10 Batch 1:  Training   loss: 0.089, accuracy: 1.0\n",
      "2017-05-17 23:51:48.489957 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 98, CIFAR-10 Batch 2:  Training   loss: 0.084, accuracy: 1.0\n",
      "2017-05-17 23:51:50.887494 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 98, CIFAR-10 Batch 3:  Training   loss: 0.057, accuracy: 1.0\n",
      "2017-05-17 23:51:53.242413 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 98, CIFAR-10 Batch 4:  Training   loss: 0.083, accuracy: 1.0\n",
      "2017-05-17 23:51:55.571128 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 98, CIFAR-10 Batch 5:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:51:57.957476 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 99, CIFAR-10 Batch 1:  Training   loss: 0.088, accuracy: 1.0\n",
      "2017-05-17 23:52:00.303298 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 99, CIFAR-10 Batch 2:  Training   loss: 0.091, accuracy: 1.0\n",
      "2017-05-17 23:52:02.685813 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 99, CIFAR-10 Batch 3:  Training   loss: 0.049, accuracy: 1.0\n",
      "2017-05-17 23:52:05.083213 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 99, CIFAR-10 Batch 4:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:52:07.475824 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 99, CIFAR-10 Batch 5:  Training   loss: 0.049, accuracy: 1.0\n",
      "2017-05-17 23:52:09.828182 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 100, CIFAR-10 Batch 1:  Training   loss: 0.08, accuracy: 1.0\n",
      "2017-05-17 23:52:12.208227 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 100, CIFAR-10 Batch 2:  Training   loss: 0.08, accuracy: 0.97\n",
      "2017-05-17 23:52:14.578194 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 100, CIFAR-10 Batch 3:  Training   loss: 0.053, accuracy: 1.0\n",
      "2017-05-17 23:52:16.927102 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 100, CIFAR-10 Batch 4:  Training   loss: 0.053, accuracy: 1.0\n",
      "2017-05-17 23:52:19.299073 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 100, CIFAR-10 Batch 5:  Training   loss: 0.053, accuracy: 1.0\n",
      "2017-05-17 23:52:21.922532 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 101, CIFAR-10 Batch 1:  Training   loss: 0.081, accuracy: 1.0\n",
      "2017-05-17 23:52:24.583181 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 101, CIFAR-10 Batch 2:  Training   loss: 0.073, accuracy: 1.0\n",
      "2017-05-17 23:52:26.843215 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 101, CIFAR-10 Batch 3:  Training   loss: 0.062, accuracy: 1.0\n",
      "2017-05-17 23:52:29.261206 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 101, CIFAR-10 Batch 4:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:52:31.639439 Validation loss: 0.89, accuracy: 0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, CIFAR-10 Batch 5:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:52:33.968771 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 102, CIFAR-10 Batch 1:  Training   loss: 0.075, accuracy: 1.0\n",
      "2017-05-17 23:52:36.332112 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 102, CIFAR-10 Batch 2:  Training   loss: 0.079, accuracy: 1.0\n",
      "2017-05-17 23:52:38.718600 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 102, CIFAR-10 Batch 3:  Training   loss: 0.063, accuracy: 1.0\n",
      "2017-05-17 23:52:41.125656 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 102, CIFAR-10 Batch 4:  Training   loss: 0.077, accuracy: 1.0\n",
      "2017-05-17 23:52:43.465524 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 102, CIFAR-10 Batch 5:  Training   loss: 0.063, accuracy: 1.0\n",
      "2017-05-17 23:52:45.849290 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 103, CIFAR-10 Batch 1:  Training   loss: 0.086, accuracy: 0.97\n",
      "2017-05-17 23:52:48.269357 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 103, CIFAR-10 Batch 2:  Training   loss: 0.08, accuracy: 1.0\n",
      "2017-05-17 23:52:50.642939 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 103, CIFAR-10 Batch 3:  Training   loss: 0.063, accuracy: 1.0\n",
      "2017-05-17 23:52:53.016098 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 103, CIFAR-10 Batch 4:  Training   loss: 0.077, accuracy: 1.0\n",
      "2017-05-17 23:52:55.379570 Validation loss: 0.87, accuracy: 0.71\n",
      "Epoch 103, CIFAR-10 Batch 5:  Training   loss: 0.042, accuracy: 1.0\n",
      "2017-05-17 23:52:57.750893 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 104, CIFAR-10 Batch 1:  Training   loss: 0.07, accuracy: 1.0\n",
      "2017-05-17 23:53:00.125900 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 104, CIFAR-10 Batch 2:  Training   loss: 0.085, accuracy: 0.98\n",
      "2017-05-17 23:53:02.515102 Validation loss: 0.93, accuracy: 0.71\n",
      "Epoch 104, CIFAR-10 Batch 3:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:53:04.881354 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 104, CIFAR-10 Batch 4:  Training   loss: 0.07, accuracy: 1.0\n",
      "2017-05-17 23:53:07.255373 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 104, CIFAR-10 Batch 5:  Training   loss: 0.045, accuracy: 1.0\n",
      "2017-05-17 23:53:09.600532 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 105, CIFAR-10 Batch 1:  Training   loss: 0.084, accuracy: 0.97\n",
      "2017-05-17 23:53:12.004195 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 105, CIFAR-10 Batch 2:  Training   loss: 0.075, accuracy: 1.0\n",
      "2017-05-17 23:53:14.370352 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 105, CIFAR-10 Batch 3:  Training   loss: 0.038, accuracy: 1.0\n",
      "2017-05-17 23:53:16.697744 Validation loss: 0.93, accuracy: 0.71\n",
      "Epoch 105, CIFAR-10 Batch 4:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:53:19.091130 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 105, CIFAR-10 Batch 5:  Training   loss: 0.034, accuracy: 1.0\n",
      "2017-05-17 23:53:21.477482 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 106, CIFAR-10 Batch 1:  Training   loss: 0.071, accuracy: 1.0\n",
      "2017-05-17 23:53:23.833784 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 106, CIFAR-10 Batch 2:  Training   loss: 0.073, accuracy: 1.0\n",
      "2017-05-17 23:53:26.163715 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 106, CIFAR-10 Batch 3:  Training   loss: 0.053, accuracy: 1.0\n",
      "2017-05-17 23:53:28.522634 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 106, CIFAR-10 Batch 4:  Training   loss: 0.094, accuracy: 0.97\n",
      "2017-05-17 23:53:30.987551 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 106, CIFAR-10 Batch 5:  Training   loss: 0.032, accuracy: 1.0\n",
      "2017-05-17 23:53:33.373361 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 107, CIFAR-10 Batch 1:  Training   loss: 0.068, accuracy: 0.97\n",
      "2017-05-17 23:53:35.741326 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 107, CIFAR-10 Batch 2:  Training   loss: 0.079, accuracy: 1.0\n",
      "2017-05-17 23:53:38.081849 Validation loss: 0.95, accuracy: 0.7\n",
      "Epoch 107, CIFAR-10 Batch 3:  Training   loss: 0.056, accuracy: 1.0\n",
      "2017-05-17 23:53:40.489487 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 107, CIFAR-10 Batch 4:  Training   loss: 0.071, accuracy: 1.0\n",
      "2017-05-17 23:53:42.849787 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 107, CIFAR-10 Batch 5:  Training   loss: 0.061, accuracy: 1.0\n",
      "2017-05-17 23:53:45.224967 Validation loss: 0.94, accuracy: 0.71\n",
      "Epoch 108, CIFAR-10 Batch 1:  Training   loss: 0.072, accuracy: 1.0\n",
      "2017-05-17 23:53:47.576520 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 108, CIFAR-10 Batch 2:  Training   loss: 0.072, accuracy: 0.97\n",
      "2017-05-17 23:53:49.953843 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 108, CIFAR-10 Batch 3:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:53:52.345135 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 108, CIFAR-10 Batch 4:  Training   loss: 0.049, accuracy: 1.0\n",
      "2017-05-17 23:53:54.731261 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 108, CIFAR-10 Batch 5:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:53:57.081060 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 109, CIFAR-10 Batch 1:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:53:59.460009 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 109, CIFAR-10 Batch 2:  Training   loss: 0.077, accuracy: 1.0\n",
      "2017-05-17 23:54:01.838157 Validation loss: 0.92, accuracy: 0.69\n",
      "Epoch 109, CIFAR-10 Batch 3:  Training   loss: 0.044, accuracy: 1.0\n",
      "2017-05-17 23:54:04.219957 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 109, CIFAR-10 Batch 4:  Training   loss: 0.06, accuracy: 1.0\n",
      "2017-05-17 23:54:06.543913 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 109, CIFAR-10 Batch 5:  Training   loss: 0.07, accuracy: 1.0\n",
      "2017-05-17 23:54:08.910087 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 110, CIFAR-10 Batch 1:  Training   loss: 0.061, accuracy: 1.0\n",
      "2017-05-17 23:54:11.274650 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 110, CIFAR-10 Batch 2:  Training   loss: 0.071, accuracy: 1.0\n",
      "2017-05-17 23:54:13.667249 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 110, CIFAR-10 Batch 3:  Training   loss: 0.05, accuracy: 1.0\n",
      "2017-05-17 23:54:16.028937 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 110, CIFAR-10 Batch 4:  Training   loss: 0.072, accuracy: 1.0\n",
      "2017-05-17 23:54:18.425894 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 110, CIFAR-10 Batch 5:  Training   loss: 0.045, accuracy: 1.0\n",
      "2017-05-17 23:54:20.846548 Validation loss: 0.87, accuracy: 0.71\n",
      "Epoch 111, CIFAR-10 Batch 1:  Training   loss: 0.093, accuracy: 0.95\n",
      "2017-05-17 23:54:23.179399 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 111, CIFAR-10 Batch 2:  Training   loss: 0.086, accuracy: 1.0\n",
      "2017-05-17 23:54:25.584668 Validation loss: 0.91, accuracy: 0.69\n",
      "Epoch 111, CIFAR-10 Batch 3:  Training   loss: 0.04, accuracy: 1.0\n",
      "2017-05-17 23:54:27.938313 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 111, CIFAR-10 Batch 4:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:54:30.278302 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 111, CIFAR-10 Batch 5:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:54:32.674945 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 112, CIFAR-10 Batch 1:  Training   loss: 0.085, accuracy: 1.0\n",
      "2017-05-17 23:54:35.039462 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 112, CIFAR-10 Batch 2:  Training   loss: 0.051, accuracy: 1.0\n",
      "2017-05-17 23:54:37.419221 Validation loss: 0.87, accuracy: 0.71\n",
      "Epoch 112, CIFAR-10 Batch 3:  Training   loss: 0.049, accuracy: 1.0\n",
      "2017-05-17 23:54:39.826939 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 112, CIFAR-10 Batch 4:  Training   loss: 0.079, accuracy: 1.0\n",
      "2017-05-17 23:54:42.231821 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 112, CIFAR-10 Batch 5:  Training   loss: 0.035, accuracy: 1.0\n",
      "2017-05-17 23:54:44.608514 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 113, CIFAR-10 Batch 1:  Training   loss: 0.086, accuracy: 1.0\n",
      "2017-05-17 23:54:46.957004 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 113, CIFAR-10 Batch 2:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:54:49.338092 Validation loss: 0.9, accuracy: 0.7\n",
      "Epoch 113, CIFAR-10 Batch 3:  Training   loss: 0.036, accuracy: 1.0\n",
      "2017-05-17 23:54:51.739906 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 113, CIFAR-10 Batch 4:  Training   loss: 0.069, accuracy: 1.0\n",
      "2017-05-17 23:54:54.098990 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 113, CIFAR-10 Batch 5:  Training   loss: 0.037, accuracy: 1.0\n",
      "2017-05-17 23:54:56.433456 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 114, CIFAR-10 Batch 1:  Training   loss: 0.067, accuracy: 1.0\n",
      "2017-05-17 23:54:58.808203 Validation loss: 0.92, accuracy: 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114, CIFAR-10 Batch 2:  Training   loss: 0.077, accuracy: 0.98\n",
      "2017-05-17 23:55:01.171695 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 114, CIFAR-10 Batch 3:  Training   loss: 0.045, accuracy: 1.0\n",
      "2017-05-17 23:55:03.526156 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 114, CIFAR-10 Batch 4:  Training   loss: 0.099, accuracy: 1.0\n",
      "2017-05-17 23:55:05.898082 Validation loss: 0.98, accuracy: 0.7\n",
      "Epoch 114, CIFAR-10 Batch 5:  Training   loss: 0.038, accuracy: 1.0\n",
      "2017-05-17 23:55:08.226230 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 115, CIFAR-10 Batch 1:  Training   loss: 0.068, accuracy: 0.97\n",
      "2017-05-17 23:55:10.590126 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 115, CIFAR-10 Batch 2:  Training   loss: 0.058, accuracy: 1.0\n",
      "2017-05-17 23:55:12.956454 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 115, CIFAR-10 Batch 3:  Training   loss: 0.04, accuracy: 1.0\n",
      "2017-05-17 23:55:15.349750 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 115, CIFAR-10 Batch 4:  Training   loss: 0.06, accuracy: 1.0\n",
      "2017-05-17 23:55:17.724192 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 115, CIFAR-10 Batch 5:  Training   loss: 0.039, accuracy: 1.0\n",
      "2017-05-17 23:55:20.103241 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 116, CIFAR-10 Batch 1:  Training   loss: 0.062, accuracy: 1.0\n",
      "2017-05-17 23:55:22.452987 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 116, CIFAR-10 Batch 2:  Training   loss: 0.09, accuracy: 0.97\n",
      "2017-05-17 23:55:24.821488 Validation loss: 0.91, accuracy: 0.7\n",
      "Epoch 116, CIFAR-10 Batch 3:  Training   loss: 0.039, accuracy: 1.0\n",
      "2017-05-17 23:55:27.205397 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 116, CIFAR-10 Batch 4:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:55:29.546068 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 116, CIFAR-10 Batch 5:  Training   loss: 0.027, accuracy: 1.0\n",
      "2017-05-17 23:55:31.937336 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 117, CIFAR-10 Batch 1:  Training   loss: 0.061, accuracy: 1.0\n",
      "2017-05-17 23:55:34.342718 Validation loss: 0.93, accuracy: 0.71\n",
      "Epoch 117, CIFAR-10 Batch 2:  Training   loss: 0.052, accuracy: 1.0\n",
      "2017-05-17 23:55:36.710690 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 117, CIFAR-10 Batch 3:  Training   loss: 0.032, accuracy: 1.0\n",
      "2017-05-17 23:55:39.227198 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 117, CIFAR-10 Batch 4:  Training   loss: 0.04, accuracy: 1.0\n",
      "2017-05-17 23:55:41.626534 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 117, CIFAR-10 Batch 5:  Training   loss: 0.029, accuracy: 1.0\n",
      "2017-05-17 23:55:44.002107 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 118, CIFAR-10 Batch 1:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:55:46.369140 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 118, CIFAR-10 Batch 2:  Training   loss: 0.065, accuracy: 1.0\n",
      "2017-05-17 23:55:48.747078 Validation loss: 0.93, accuracy: 0.7\n",
      "Epoch 118, CIFAR-10 Batch 3:  Training   loss: 0.033, accuracy: 1.0\n",
      "2017-05-17 23:55:51.137052 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 118, CIFAR-10 Batch 4:  Training   loss: 0.04, accuracy: 1.0\n",
      "2017-05-17 23:55:53.544367 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 118, CIFAR-10 Batch 5:  Training   loss: 0.038, accuracy: 1.0\n",
      "2017-05-17 23:55:55.887777 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 119, CIFAR-10 Batch 1:  Training   loss: 0.073, accuracy: 1.0\n",
      "2017-05-17 23:55:58.256359 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 119, CIFAR-10 Batch 2:  Training   loss: 0.065, accuracy: 0.97\n",
      "2017-05-17 23:56:00.653105 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 119, CIFAR-10 Batch 3:  Training   loss: 0.049, accuracy: 1.0\n",
      "2017-05-17 23:56:03.038059 Validation loss: 0.97, accuracy: 0.7\n",
      "Epoch 119, CIFAR-10 Batch 4:  Training   loss: 0.05, accuracy: 1.0\n",
      "2017-05-17 23:56:05.454152 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 119, CIFAR-10 Batch 5:  Training   loss: 0.045, accuracy: 1.0\n",
      "2017-05-17 23:56:07.865039 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 120, CIFAR-10 Batch 1:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:56:10.228529 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 120, CIFAR-10 Batch 2:  Training   loss: 0.064, accuracy: 1.0\n",
      "2017-05-17 23:56:12.600344 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 120, CIFAR-10 Batch 3:  Training   loss: 0.035, accuracy: 1.0\n",
      "2017-05-17 23:56:14.960461 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 120, CIFAR-10 Batch 4:  Training   loss: 0.043, accuracy: 1.0\n",
      "2017-05-17 23:56:17.343967 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 120, CIFAR-10 Batch 5:  Training   loss: 0.032, accuracy: 1.0\n",
      "2017-05-17 23:56:19.711402 Validation loss: 0.89, accuracy: 0.71\n",
      "Epoch 121, CIFAR-10 Batch 1:  Training   loss: 0.05, accuracy: 1.0\n",
      "2017-05-17 23:56:22.073931 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 121, CIFAR-10 Batch 2:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:56:24.453866 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 121, CIFAR-10 Batch 3:  Training   loss: 0.039, accuracy: 1.0\n",
      "2017-05-17 23:56:26.842200 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 121, CIFAR-10 Batch 4:  Training   loss: 0.043, accuracy: 1.0\n",
      "2017-05-17 23:56:29.263251 Validation loss: 0.88, accuracy: 0.71\n",
      "Epoch 121, CIFAR-10 Batch 5:  Training   loss: 0.027, accuracy: 1.0\n",
      "2017-05-17 23:56:31.625012 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 122, CIFAR-10 Batch 1:  Training   loss: 0.055, accuracy: 1.0\n",
      "2017-05-17 23:56:33.952471 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 122, CIFAR-10 Batch 2:  Training   loss: 0.069, accuracy: 1.0\n",
      "2017-05-17 23:56:36.327918 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 122, CIFAR-10 Batch 3:  Training   loss: 0.045, accuracy: 1.0\n",
      "2017-05-17 23:56:38.675158 Validation loss: 0.94, accuracy: 0.71\n",
      "Epoch 122, CIFAR-10 Batch 4:  Training   loss: 0.043, accuracy: 1.0\n",
      "2017-05-17 23:56:41.068582 Validation loss: 0.86, accuracy: 0.72\n",
      "Epoch 122, CIFAR-10 Batch 5:  Training   loss: 0.035, accuracy: 1.0\n",
      "2017-05-17 23:56:43.453572 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 123, CIFAR-10 Batch 1:  Training   loss: 0.065, accuracy: 1.0\n",
      "2017-05-17 23:56:45.817362 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 123, CIFAR-10 Batch 2:  Training   loss: 0.085, accuracy: 0.97\n",
      "2017-05-17 23:56:48.185324 Validation loss: 0.97, accuracy: 0.69\n",
      "Epoch 123, CIFAR-10 Batch 3:  Training   loss: 0.048, accuracy: 1.0\n",
      "2017-05-17 23:56:50.522558 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 123, CIFAR-10 Batch 4:  Training   loss: 0.048, accuracy: 1.0\n",
      "2017-05-17 23:56:52.935414 Validation loss: 0.94, accuracy: 0.7\n",
      "Epoch 123, CIFAR-10 Batch 5:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:56:55.365067 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 124, CIFAR-10 Batch 1:  Training   loss: 0.054, accuracy: 1.0\n",
      "2017-05-17 23:56:57.767160 Validation loss: 0.9, accuracy: 0.71\n",
      "Epoch 124, CIFAR-10 Batch 2:  Training   loss: 0.051, accuracy: 1.0\n",
      "2017-05-17 23:57:00.149717 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 124, CIFAR-10 Batch 3:  Training   loss: 0.037, accuracy: 1.0\n",
      "2017-05-17 23:57:02.516788 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 124, CIFAR-10 Batch 4:  Training   loss: 0.053, accuracy: 1.0\n",
      "2017-05-17 23:57:04.860935 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 124, CIFAR-10 Batch 5:  Training   loss: 0.033, accuracy: 1.0\n",
      "2017-05-17 23:57:07.198028 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 125, CIFAR-10 Batch 1:  Training   loss: 0.051, accuracy: 1.0\n",
      "2017-05-17 23:57:09.591246 Validation loss: 0.94, accuracy: 0.71\n",
      "Epoch 125, CIFAR-10 Batch 2:  Training   loss: 0.048, accuracy: 1.0\n",
      "2017-05-17 23:57:11.974910 Validation loss: 0.92, accuracy: 0.7\n",
      "Epoch 125, CIFAR-10 Batch 3:  Training   loss: 0.044, accuracy: 1.0\n",
      "2017-05-17 23:57:14.345466 Validation loss: 0.92, accuracy: 0.71\n",
      "Epoch 125, CIFAR-10 Batch 4:  Training   loss: 0.036, accuracy: 1.0\n",
      "2017-05-17 23:57:16.734029 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 125, CIFAR-10 Batch 5:  Training   loss: 0.042, accuracy: 1.0\n",
      "2017-05-17 23:57:19.112323 Validation loss: 0.91, accuracy: 0.71\n",
      "Epoch 126, CIFAR-10 Batch 1:  Training   loss: 0.045, accuracy: 1.0\n",
      "2017-05-17 23:57:21.479104 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 126, CIFAR-10 Batch 2:  Training   loss: 0.043, accuracy: 1.0\n",
      "2017-05-17 23:57:23.848364 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 126, CIFAR-10 Batch 3:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-17 23:57:26.201523 Validation loss: 0.88, accuracy: 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, CIFAR-10 Batch 4:  Training   loss: 0.041, accuracy: 1.0\n",
      "2017-05-17 23:57:28.594795 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 126, CIFAR-10 Batch 5:  Training   loss: 0.033, accuracy: 1.0\n",
      "2017-05-17 23:57:30.993403 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 127, CIFAR-10 Batch 1:  Training   loss: 0.042, accuracy: 1.0\n",
      "2017-05-17 23:57:33.421214 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 127, CIFAR-10 Batch 2:  Training   loss: 0.03, accuracy: 1.0\n",
      "2017-05-17 23:57:35.837807 Validation loss: 0.88, accuracy: 0.72\n",
      "Epoch 127, CIFAR-10 Batch 3:  Training   loss: 0.031, accuracy: 1.0\n",
      "2017-05-17 23:57:38.197269 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 127, CIFAR-10 Batch 4:  Training   loss: 0.032, accuracy: 1.0\n",
      "2017-05-17 23:57:40.555543 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 127, CIFAR-10 Batch 5:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:57:42.922807 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 128, CIFAR-10 Batch 1:  Training   loss: 0.032, accuracy: 1.0\n",
      "2017-05-17 23:57:45.286855 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 128, CIFAR-10 Batch 2:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-17 23:57:47.690474 Validation loss: 0.88, accuracy: 0.73\n",
      "Epoch 128, CIFAR-10 Batch 3:  Training   loss: 0.025, accuracy: 1.0\n",
      "2017-05-17 23:57:50.114445 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 128, CIFAR-10 Batch 4:  Training   loss: 0.03, accuracy: 1.0\n",
      "2017-05-17 23:57:52.518444 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 128, CIFAR-10 Batch 5:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:57:54.894595 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 129, CIFAR-10 Batch 1:  Training   loss: 0.027, accuracy: 1.0\n",
      "2017-05-17 23:57:57.241761 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 129, CIFAR-10 Batch 2:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:57:59.595688 Validation loss: 0.88, accuracy: 0.73\n",
      "Epoch 129, CIFAR-10 Batch 3:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-17 23:58:01.972886 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 129, CIFAR-10 Batch 4:  Training   loss: 0.027, accuracy: 1.0\n",
      "2017-05-17 23:58:04.339451 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 129, CIFAR-10 Batch 5:  Training   loss: 0.025, accuracy: 1.0\n",
      "2017-05-17 23:58:06.742902 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 130, CIFAR-10 Batch 1:  Training   loss: 0.033, accuracy: 1.0\n",
      "2017-05-17 23:58:09.113738 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 130, CIFAR-10 Batch 2:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:58:11.484135 Validation loss: 0.88, accuracy: 0.73\n",
      "Epoch 130, CIFAR-10 Batch 3:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-17 23:58:13.890816 Validation loss: 0.87, accuracy: 0.72\n",
      "Epoch 130, CIFAR-10 Batch 4:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-17 23:58:16.235285 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 130, CIFAR-10 Batch 5:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-17 23:58:18.570625 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 131, CIFAR-10 Batch 1:  Training   loss: 0.035, accuracy: 1.0\n",
      "2017-05-17 23:58:20.936289 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 131, CIFAR-10 Batch 2:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:58:23.316049 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 131, CIFAR-10 Batch 3:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-17 23:58:25.694335 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 131, CIFAR-10 Batch 4:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:58:28.098559 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 131, CIFAR-10 Batch 5:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-17 23:58:30.513816 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 132, CIFAR-10 Batch 1:  Training   loss: 0.025, accuracy: 1.0\n",
      "2017-05-17 23:58:32.890485 Validation loss: 0.94, accuracy: 0.72\n",
      "Epoch 132, CIFAR-10 Batch 2:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:58:35.225518 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 132, CIFAR-10 Batch 3:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-17 23:58:37.581501 Validation loss: 0.88, accuracy: 0.73\n",
      "Epoch 132, CIFAR-10 Batch 4:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-17 23:58:39.947458 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 132, CIFAR-10 Batch 5:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-17 23:58:42.338442 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 133, CIFAR-10 Batch 1:  Training   loss: 0.028, accuracy: 1.0\n",
      "2017-05-17 23:58:44.693832 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 133, CIFAR-10 Batch 2:  Training   loss: 0.025, accuracy: 1.0\n",
      "2017-05-17 23:58:47.041015 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 133, CIFAR-10 Batch 3:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-17 23:58:49.427539 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 133, CIFAR-10 Batch 4:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:58:51.857183 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 133, CIFAR-10 Batch 5:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-17 23:58:54.260623 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 134, CIFAR-10 Batch 1:  Training   loss: 0.027, accuracy: 1.0\n",
      "2017-05-17 23:58:56.607166 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 134, CIFAR-10 Batch 2:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:58:58.941476 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 134, CIFAR-10 Batch 3:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-17 23:59:01.288125 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 134, CIFAR-10 Batch 4:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-17 23:59:03.674575 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 134, CIFAR-10 Batch 5:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-17 23:59:06.064692 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 135, CIFAR-10 Batch 1:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:59:08.401457 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 135, CIFAR-10 Batch 2:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-17 23:59:10.780151 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 135, CIFAR-10 Batch 3:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-17 23:59:13.118313 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 135, CIFAR-10 Batch 4:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:59:15.430386 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 135, CIFAR-10 Batch 5:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-17 23:59:17.814571 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 136, CIFAR-10 Batch 1:  Training   loss: 0.036, accuracy: 1.0\n",
      "2017-05-17 23:59:20.163645 Validation loss: 0.95, accuracy: 0.72\n",
      "Epoch 136, CIFAR-10 Batch 2:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:59:22.545738 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 136, CIFAR-10 Batch 3:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-17 23:59:24.884074 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 136, CIFAR-10 Batch 4:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-17 23:59:27.257719 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 136, CIFAR-10 Batch 5:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:59:29.654085 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 137, CIFAR-10 Batch 1:  Training   loss: 0.027, accuracy: 1.0\n",
      "2017-05-17 23:59:32.027195 Validation loss: 0.94, accuracy: 0.72\n",
      "Epoch 137, CIFAR-10 Batch 2:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-17 23:59:34.440696 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 137, CIFAR-10 Batch 3:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-17 23:59:36.855885 Validation loss: 0.89, accuracy: 0.72\n",
      "Epoch 137, CIFAR-10 Batch 4:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-17 23:59:39.198528 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 137, CIFAR-10 Batch 5:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:59:41.572147 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 138, CIFAR-10 Batch 1:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-17 23:59:43.954892 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 138, CIFAR-10 Batch 2:  Training   loss: 0.025, accuracy: 1.0\n",
      "2017-05-17 23:59:46.342468 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 138, CIFAR-10 Batch 3:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-17 23:59:48.704074 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 138, CIFAR-10 Batch 4:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-17 23:59:51.099026 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 138, CIFAR-10 Batch 5:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-17 23:59:53.446806 Validation loss: 0.89, accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139, CIFAR-10 Batch 1:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:59:55.829117 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 139, CIFAR-10 Batch 2:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-17 23:59:58.223270 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 139, CIFAR-10 Batch 3:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:00:00.609896 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 139, CIFAR-10 Batch 4:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-18 00:00:02.965467 Validation loss: 0.94, accuracy: 0.72\n",
      "Epoch 139, CIFAR-10 Batch 5:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:00:05.357000 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 140, CIFAR-10 Batch 1:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-18 00:00:07.750866 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 140, CIFAR-10 Batch 2:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-18 00:00:10.098596 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 140, CIFAR-10 Batch 3:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:00:12.486793 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 140, CIFAR-10 Batch 4:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:00:14.853075 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 140, CIFAR-10 Batch 5:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:00:17.205142 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 141, CIFAR-10 Batch 1:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:00:19.575966 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 141, CIFAR-10 Batch 2:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:00:21.953390 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 141, CIFAR-10 Batch 3:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:00:24.361412 Validation loss: 0.88, accuracy: 0.73\n",
      "Epoch 141, CIFAR-10 Batch 4:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:00:26.708493 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 141, CIFAR-10 Batch 5:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:00:29.082771 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 142, CIFAR-10 Batch 1:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:00:31.454296 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 142, CIFAR-10 Batch 2:  Training   loss: 0.024, accuracy: 1.0\n",
      "2017-05-18 00:00:33.815187 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 142, CIFAR-10 Batch 3:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:00:36.200347 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 142, CIFAR-10 Batch 4:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:00:38.575587 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 142, CIFAR-10 Batch 5:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:00:40.952449 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 143, CIFAR-10 Batch 1:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:00:43.360916 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 143, CIFAR-10 Batch 2:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:00:45.733006 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 143, CIFAR-10 Batch 3:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:00:48.127441 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 143, CIFAR-10 Batch 4:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:00:50.492152 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 143, CIFAR-10 Batch 5:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:00:52.849436 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 144, CIFAR-10 Batch 1:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:00:55.215174 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 144, CIFAR-10 Batch 2:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:00:57.536607 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 144, CIFAR-10 Batch 3:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:00:59.892762 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 144, CIFAR-10 Batch 4:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:01:02.316525 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 144, CIFAR-10 Batch 5:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:01:04.739096 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 145, CIFAR-10 Batch 1:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-18 00:01:07.092435 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 145, CIFAR-10 Batch 2:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-18 00:01:09.440982 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 145, CIFAR-10 Batch 3:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:01:11.822614 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 145, CIFAR-10 Batch 4:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:01:14.174096 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 145, CIFAR-10 Batch 5:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:01:16.595944 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 146, CIFAR-10 Batch 1:  Training   loss: 0.025, accuracy: 1.0\n",
      "2017-05-18 00:01:18.977530 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 146, CIFAR-10 Batch 2:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:01:21.345426 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 146, CIFAR-10 Batch 3:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:01:23.706580 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 146, CIFAR-10 Batch 4:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:01:26.095985 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 146, CIFAR-10 Batch 5:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:01:28.446470 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 147, CIFAR-10 Batch 1:  Training   loss: 0.028, accuracy: 1.0\n",
      "2017-05-18 00:01:30.781357 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 147, CIFAR-10 Batch 2:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:01:33.164505 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 147, CIFAR-10 Batch 3:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:01:35.572038 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 147, CIFAR-10 Batch 4:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:01:37.978949 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 147, CIFAR-10 Batch 5:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:01:40.411323 Validation loss: 0.88, accuracy: 0.73\n",
      "Epoch 148, CIFAR-10 Batch 1:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:01:42.795254 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 148, CIFAR-10 Batch 2:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:01:45.128501 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 148, CIFAR-10 Batch 3:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:01:47.480882 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 148, CIFAR-10 Batch 4:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:01:49.872260 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 148, CIFAR-10 Batch 5:  Training   loss: 0.023, accuracy: 1.0\n",
      "2017-05-18 00:01:52.247050 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 149, CIFAR-10 Batch 1:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:01:54.613048 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 149, CIFAR-10 Batch 2:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:01:56.992078 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 149, CIFAR-10 Batch 3:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:01:59.327137 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 149, CIFAR-10 Batch 4:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:02:01.699887 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 149, CIFAR-10 Batch 5:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:02:04.097910 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 150, CIFAR-10 Batch 1:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:02:06.482600 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 150, CIFAR-10 Batch 2:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:02:08.820299 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 150, CIFAR-10 Batch 3:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:02:11.154142 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 150, CIFAR-10 Batch 4:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:02:13.515354 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 150, CIFAR-10 Batch 5:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:02:15.908793 Validation loss: 0.89, accuracy: 0.73\n",
      "Epoch 151, CIFAR-10 Batch 1:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:02:18.257496 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 151, CIFAR-10 Batch 2:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:02:20.626001 Validation loss: 0.92, accuracy: 0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, CIFAR-10 Batch 3:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:02:22.934606 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 151, CIFAR-10 Batch 4:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:02:25.305572 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 151, CIFAR-10 Batch 5:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:02:27.712634 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 152, CIFAR-10 Batch 1:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:02:30.088728 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 152, CIFAR-10 Batch 2:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:02:32.440328 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 152, CIFAR-10 Batch 3:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:02:34.830131 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 152, CIFAR-10 Batch 4:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:02:37.162889 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 152, CIFAR-10 Batch 5:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:02:39.542949 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 153, CIFAR-10 Batch 1:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:02:41.900935 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 153, CIFAR-10 Batch 2:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:02:44.303875 Validation loss: 0.9, accuracy: 0.72\n",
      "Epoch 153, CIFAR-10 Batch 3:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:02:46.708181 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 153, CIFAR-10 Batch 4:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:02:49.066889 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 153, CIFAR-10 Batch 5:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:02:51.433582 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 154, CIFAR-10 Batch 1:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:02:53.783343 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 154, CIFAR-10 Batch 2:  Training   loss: 0.026, accuracy: 1.0\n",
      "2017-05-18 00:02:56.114999 Validation loss: 0.94, accuracy: 0.72\n",
      "Epoch 154, CIFAR-10 Batch 3:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:02:58.473064 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 154, CIFAR-10 Batch 4:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:03:00.851513 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 154, CIFAR-10 Batch 5:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:03:03.196229 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 155, CIFAR-10 Batch 1:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:03:05.590748 Validation loss: 0.96, accuracy: 0.72\n",
      "Epoch 155, CIFAR-10 Batch 2:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:03:07.958889 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 155, CIFAR-10 Batch 3:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:03:10.300170 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 155, CIFAR-10 Batch 4:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:03:12.647122 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 155, CIFAR-10 Batch 5:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:03:15.019143 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 156, CIFAR-10 Batch 1:  Training   loss: 0.02, accuracy: 1.0\n",
      "2017-05-18 00:03:17.402932 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 156, CIFAR-10 Batch 2:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:03:19.800735 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 156, CIFAR-10 Batch 3:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:03:22.156780 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 156, CIFAR-10 Batch 4:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:03:24.556128 Validation loss: 0.91, accuracy: 0.72\n",
      "Epoch 156, CIFAR-10 Batch 5:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:03:26.942987 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 157, CIFAR-10 Batch 1:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:03:29.257348 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 157, CIFAR-10 Batch 2:  Training   loss: 0.022, accuracy: 1.0\n",
      "2017-05-18 00:03:31.622698 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 157, CIFAR-10 Batch 3:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:03:34.063198 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 157, CIFAR-10 Batch 4:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:03:36.450144 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 157, CIFAR-10 Batch 5:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:03:38.811871 Validation loss: 0.92, accuracy: 0.72\n",
      "Epoch 158, CIFAR-10 Batch 1:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:03:41.181831 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 158, CIFAR-10 Batch 2:  Training   loss: 0.021, accuracy: 1.0\n",
      "2017-05-18 00:03:43.524827 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 158, CIFAR-10 Batch 3:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:03:45.893379 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 158, CIFAR-10 Batch 4:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:03:48.253480 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 158, CIFAR-10 Batch 5:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:03:50.626189 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 159, CIFAR-10 Batch 1:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:03:53.038010 Validation loss: 0.94, accuracy: 0.72\n",
      "Epoch 159, CIFAR-10 Batch 2:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:03:55.402653 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 159, CIFAR-10 Batch 3:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:03:57.745864 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 159, CIFAR-10 Batch 4:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:04:00.104453 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 159, CIFAR-10 Batch 5:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:04:02.447506 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 160, CIFAR-10 Batch 1:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:04:04.822813 Validation loss: 0.94, accuracy: 0.72\n",
      "Epoch 160, CIFAR-10 Batch 2:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:04:07.204370 Validation loss: 0.93, accuracy: 0.72\n",
      "Epoch 160, CIFAR-10 Batch 3:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:04:09.561913 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 160, CIFAR-10 Batch 4:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:04:11.896532 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 160, CIFAR-10 Batch 5:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:04:14.263422 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 161, CIFAR-10 Batch 1:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:04:16.611730 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 161, CIFAR-10 Batch 2:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:04:19.001191 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 161, CIFAR-10 Batch 3:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:04:21.379617 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 161, CIFAR-10 Batch 4:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:04:23.712674 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 161, CIFAR-10 Batch 5:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:04:26.096983 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 162, CIFAR-10 Batch 1:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:04:28.500189 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 162, CIFAR-10 Batch 2:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:04:30.866166 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 162, CIFAR-10 Batch 3:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:04:33.303704 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 162, CIFAR-10 Batch 4:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:04:35.788767 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 162, CIFAR-10 Batch 5:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:04:38.178316 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 163, CIFAR-10 Batch 1:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:04:40.531202 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 163, CIFAR-10 Batch 2:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:04:42.921483 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 163, CIFAR-10 Batch 3:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:04:45.336557 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 163, CIFAR-10 Batch 4:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:04:47.723222 Validation loss: 0.92, accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163, CIFAR-10 Batch 5:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:04:50.075793 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 164, CIFAR-10 Batch 1:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:04:52.414463 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 164, CIFAR-10 Batch 2:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:04:54.809640 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 164, CIFAR-10 Batch 3:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:04:57.192172 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 164, CIFAR-10 Batch 4:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:04:59.528557 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 164, CIFAR-10 Batch 5:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:05:01.903868 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 165, CIFAR-10 Batch 1:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:05:04.264520 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 165, CIFAR-10 Batch 2:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:05:06.673051 Validation loss: 0.91, accuracy: 0.74\n",
      "Epoch 165, CIFAR-10 Batch 3:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:05:09.084523 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 165, CIFAR-10 Batch 4:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:05:11.437376 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 165, CIFAR-10 Batch 5:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:05:13.781037 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 166, CIFAR-10 Batch 1:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:05:16.156769 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 166, CIFAR-10 Batch 2:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:05:18.502315 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 166, CIFAR-10 Batch 3:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:05:20.862139 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 166, CIFAR-10 Batch 4:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:05:23.257759 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 166, CIFAR-10 Batch 5:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:05:25.631370 Validation loss: 0.91, accuracy: 0.74\n",
      "Epoch 167, CIFAR-10 Batch 1:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:05:28.010323 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 167, CIFAR-10 Batch 2:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:05:30.365138 Validation loss: 0.91, accuracy: 0.74\n",
      "Epoch 167, CIFAR-10 Batch 3:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:05:32.732339 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 167, CIFAR-10 Batch 4:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:05:35.092996 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 167, CIFAR-10 Batch 5:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:05:37.424534 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 168, CIFAR-10 Batch 1:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:05:39.887661 Validation loss: 0.92, accuracy: 0.74\n",
      "Epoch 168, CIFAR-10 Batch 2:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:05:42.303182 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 168, CIFAR-10 Batch 3:  Training   loss: 0.0098, accuracy: 1.0\n",
      "2017-05-18 00:05:44.680420 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 168, CIFAR-10 Batch 4:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:05:47.017345 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 168, CIFAR-10 Batch 5:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:05:49.392822 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 169, CIFAR-10 Batch 1:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:05:51.746113 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 169, CIFAR-10 Batch 2:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:05:54.109195 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 169, CIFAR-10 Batch 3:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:05:56.496157 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 169, CIFAR-10 Batch 4:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:05:58.860561 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 169, CIFAR-10 Batch 5:  Training   loss: 0.0089, accuracy: 1.0\n",
      "2017-05-18 00:06:01.203603 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 170, CIFAR-10 Batch 1:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:06:03.565557 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 170, CIFAR-10 Batch 2:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:06:05.930198 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 170, CIFAR-10 Batch 3:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:06:08.303706 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 170, CIFAR-10 Batch 4:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:06:10.698699 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 170, CIFAR-10 Batch 5:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:06:13.080378 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 171, CIFAR-10 Batch 1:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:06:15.424978 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 171, CIFAR-10 Batch 2:  Training   loss: 0.019, accuracy: 1.0\n",
      "2017-05-18 00:06:17.818187 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 171, CIFAR-10 Batch 3:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:06:20.213926 Validation loss: 0.9, accuracy: 0.73\n",
      "Epoch 171, CIFAR-10 Batch 4:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:06:22.617295 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 171, CIFAR-10 Batch 5:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:06:25.039394 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 172, CIFAR-10 Batch 1:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:06:27.418231 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 172, CIFAR-10 Batch 2:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:06:29.765991 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 172, CIFAR-10 Batch 3:  Training   loss: 0.008, accuracy: 1.0\n",
      "2017-05-18 00:06:32.131540 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 172, CIFAR-10 Batch 4:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:06:34.488625 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 172, CIFAR-10 Batch 5:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:06:36.862995 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 173, CIFAR-10 Batch 1:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:06:39.201942 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 173, CIFAR-10 Batch 2:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:06:41.565312 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 173, CIFAR-10 Batch 3:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:06:43.931234 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 173, CIFAR-10 Batch 4:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:06:46.318959 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 173, CIFAR-10 Batch 5:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:06:48.704099 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 174, CIFAR-10 Batch 1:  Training   loss: 0.018, accuracy: 1.0\n",
      "2017-05-18 00:06:51.062041 Validation loss: 0.97, accuracy: 0.72\n",
      "Epoch 174, CIFAR-10 Batch 2:  Training   loss: 0.017, accuracy: 1.0\n",
      "2017-05-18 00:06:53.465112 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 174, CIFAR-10 Batch 3:  Training   loss: 0.0094, accuracy: 1.0\n",
      "2017-05-18 00:06:55.859744 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 174, CIFAR-10 Batch 4:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:06:58.260233 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 174, CIFAR-10 Batch 5:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:07:00.630000 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 175, CIFAR-10 Batch 1:  Training   loss: 0.016, accuracy: 1.0\n",
      "2017-05-18 00:07:02.992028 Validation loss: 0.96, accuracy: 0.72\n",
      "Epoch 175, CIFAR-10 Batch 2:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:07:05.370063 Validation loss: 0.91, accuracy: 0.73\n",
      "Epoch 175, CIFAR-10 Batch 3:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:07:07.747398 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 175, CIFAR-10 Batch 4:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:07:10.127176 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 175, CIFAR-10 Batch 5:  Training   loss: 0.009, accuracy: 1.0\n",
      "2017-05-18 00:07:12.539157 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 176, CIFAR-10 Batch 1:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:07:14.903186 Validation loss: 0.94, accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176, CIFAR-10 Batch 2:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:07:17.295601 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 176, CIFAR-10 Batch 3:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:07:19.685319 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 176, CIFAR-10 Batch 4:  Training   loss: 0.009, accuracy: 1.0\n",
      "2017-05-18 00:07:22.089586 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 176, CIFAR-10 Batch 5:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:07:24.439729 Validation loss: 0.92, accuracy: 0.74\n",
      "Epoch 177, CIFAR-10 Batch 1:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:07:26.774623 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 177, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:07:29.137110 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 177, CIFAR-10 Batch 3:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:07:31.478690 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 177, CIFAR-10 Batch 4:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:07:33.868296 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 177, CIFAR-10 Batch 5:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:07:36.226705 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 178, CIFAR-10 Batch 1:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:07:38.653559 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 178, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:07:41.081168 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 178, CIFAR-10 Batch 3:  Training   loss: 0.0077, accuracy: 1.0\n",
      "2017-05-18 00:07:43.482070 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 178, CIFAR-10 Batch 4:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:07:45.806179 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 178, CIFAR-10 Batch 5:  Training   loss: 0.0093, accuracy: 1.0\n",
      "2017-05-18 00:07:48.185441 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 179, CIFAR-10 Batch 1:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:07:50.535844 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 179, CIFAR-10 Batch 2:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:07:52.910299 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 179, CIFAR-10 Batch 3:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:07:55.268673 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 179, CIFAR-10 Batch 4:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:07:57.612574 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 179, CIFAR-10 Batch 5:  Training   loss: 0.0087, accuracy: 1.0\n",
      "2017-05-18 00:07:59.979834 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 180, CIFAR-10 Batch 1:  Training   loss: 0.015, accuracy: 1.0\n",
      "2017-05-18 00:08:02.281858 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 180, CIFAR-10 Batch 2:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:08:04.606056 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 180, CIFAR-10 Batch 3:  Training   loss: 0.0093, accuracy: 1.0\n",
      "2017-05-18 00:08:06.882088 Validation loss: 0.92, accuracy: 0.74\n",
      "Epoch 180, CIFAR-10 Batch 4:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:08:09.192690 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 180, CIFAR-10 Batch 5:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:08:11.482302 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 181, CIFAR-10 Batch 1:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:08:13.763111 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 181, CIFAR-10 Batch 2:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:08:16.081481 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 181, CIFAR-10 Batch 3:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:08:18.380558 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 181, CIFAR-10 Batch 4:  Training   loss: 0.0076, accuracy: 1.0\n",
      "2017-05-18 00:08:20.690176 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 181, CIFAR-10 Batch 5:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:08:23.016400 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 182, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:08:25.311200 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 182, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:08:27.600033 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 182, CIFAR-10 Batch 3:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:08:29.891296 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 182, CIFAR-10 Batch 4:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:08:32.217404 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 182, CIFAR-10 Batch 5:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:08:34.503888 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 183, CIFAR-10 Batch 1:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:08:36.803482 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 183, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:08:39.117112 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 183, CIFAR-10 Batch 3:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:08:41.444467 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 183, CIFAR-10 Batch 4:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:08:43.755714 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 183, CIFAR-10 Batch 5:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:08:46.085207 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 184, CIFAR-10 Batch 1:  Training   loss: 0.014, accuracy: 1.0\n",
      "2017-05-18 00:08:48.388469 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 184, CIFAR-10 Batch 2:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:08:50.688291 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 184, CIFAR-10 Batch 3:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:08:52.993454 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 184, CIFAR-10 Batch 4:  Training   loss: 0.009, accuracy: 1.0\n",
      "2017-05-18 00:08:55.294016 Validation loss: 0.92, accuracy: 0.73\n",
      "Epoch 184, CIFAR-10 Batch 5:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:08:57.610348 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 185, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:08:59.975510 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 185, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:02.307975 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 185, CIFAR-10 Batch 3:  Training   loss: 0.0073, accuracy: 1.0\n",
      "2017-05-18 00:09:04.651917 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 185, CIFAR-10 Batch 4:  Training   loss: 0.0094, accuracy: 1.0\n",
      "2017-05-18 00:09:07.006523 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 185, CIFAR-10 Batch 5:  Training   loss: 0.0087, accuracy: 1.0\n",
      "2017-05-18 00:09:09.333238 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 186, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:11.684504 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 186, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:14.056595 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 186, CIFAR-10 Batch 3:  Training   loss: 0.008, accuracy: 1.0\n",
      "2017-05-18 00:09:16.446840 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 186, CIFAR-10 Batch 4:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:09:18.825404 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 186, CIFAR-10 Batch 5:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:09:21.181754 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 187, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:23.524836 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 187, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:25.906960 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 187, CIFAR-10 Batch 3:  Training   loss: 0.007, accuracy: 1.0\n",
      "2017-05-18 00:09:28.293145 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 187, CIFAR-10 Batch 4:  Training   loss: 0.009, accuracy: 1.0\n",
      "2017-05-18 00:09:30.650704 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 187, CIFAR-10 Batch 5:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:09:33.026672 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 188, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:09:35.424003 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 188, CIFAR-10 Batch 2:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:09:37.819094 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 188, CIFAR-10 Batch 3:  Training   loss: 0.008, accuracy: 1.0\n",
      "2017-05-18 00:09:40.196611 Validation loss: 0.94, accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188, CIFAR-10 Batch 4:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:09:42.572018 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 188, CIFAR-10 Batch 5:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:09:44.938245 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 189, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:47.333670 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 189, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:49.709047 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 189, CIFAR-10 Batch 3:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:09:52.109326 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 189, CIFAR-10 Batch 4:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:09:54.510736 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 189, CIFAR-10 Batch 5:  Training   loss: 0.0075, accuracy: 1.0\n",
      "2017-05-18 00:09:56.889340 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 190, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:09:59.286125 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 190, CIFAR-10 Batch 2:  Training   loss: 0.013, accuracy: 1.0\n",
      "2017-05-18 00:10:01.660100 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 190, CIFAR-10 Batch 3:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:10:04.048792 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 190, CIFAR-10 Batch 4:  Training   loss: 0.0094, accuracy: 1.0\n",
      "2017-05-18 00:10:06.439011 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 190, CIFAR-10 Batch 5:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:10:08.802766 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 191, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:10:11.205973 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 191, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:10:13.599110 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 191, CIFAR-10 Batch 3:  Training   loss: 0.0074, accuracy: 1.0\n",
      "2017-05-18 00:10:15.988741 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 191, CIFAR-10 Batch 4:  Training   loss: 0.0095, accuracy: 1.0\n",
      "2017-05-18 00:10:18.341246 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 191, CIFAR-10 Batch 5:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:10:20.742157 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 192, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:10:23.136430 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 192, CIFAR-10 Batch 2:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:10:25.536721 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 192, CIFAR-10 Batch 3:  Training   loss: 0.007, accuracy: 1.0\n",
      "2017-05-18 00:10:27.939539 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 192, CIFAR-10 Batch 4:  Training   loss: 0.0072, accuracy: 1.0\n",
      "2017-05-18 00:10:30.307897 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 192, CIFAR-10 Batch 5:  Training   loss: 0.0074, accuracy: 1.0\n",
      "2017-05-18 00:10:32.683757 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 193, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:10:35.085046 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 193, CIFAR-10 Batch 2:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:10:37.464934 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 193, CIFAR-10 Batch 3:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:10:39.860969 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 193, CIFAR-10 Batch 4:  Training   loss: 0.0094, accuracy: 1.0\n",
      "2017-05-18 00:10:42.276819 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 193, CIFAR-10 Batch 5:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:10:44.672919 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 194, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:10:47.071188 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 194, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:10:49.428302 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 194, CIFAR-10 Batch 3:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:10:51.812060 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 194, CIFAR-10 Batch 4:  Training   loss: 0.0095, accuracy: 1.0\n",
      "2017-05-18 00:10:54.211753 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 194, CIFAR-10 Batch 5:  Training   loss: 0.0089, accuracy: 1.0\n",
      "2017-05-18 00:10:56.615439 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 195, CIFAR-10 Batch 1:  Training   loss: 0.012, accuracy: 1.0\n",
      "2017-05-18 00:10:59.009820 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 195, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:11:01.415490 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 195, CIFAR-10 Batch 3:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:11:03.853731 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 195, CIFAR-10 Batch 4:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:11:06.251336 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 195, CIFAR-10 Batch 5:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:11:08.616992 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 196, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:11:11.007173 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 196, CIFAR-10 Batch 2:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:11:13.381133 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 196, CIFAR-10 Batch 3:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:11:15.773446 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 196, CIFAR-10 Batch 4:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:11:18.198087 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 196, CIFAR-10 Batch 5:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:11:20.572389 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 197, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:11:22.962969 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 197, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:11:25.370101 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 197, CIFAR-10 Batch 3:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:11:27.772010 Validation loss: 0.96, accuracy: 0.74\n",
      "Epoch 197, CIFAR-10 Batch 4:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:11:30.174084 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 197, CIFAR-10 Batch 5:  Training   loss: 0.0075, accuracy: 1.0\n",
      "2017-05-18 00:11:32.549406 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 198, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:11:34.924318 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 198, CIFAR-10 Batch 2:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:11:37.298454 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 198, CIFAR-10 Batch 3:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:11:39.680795 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 198, CIFAR-10 Batch 4:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:11:42.080617 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 198, CIFAR-10 Batch 5:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:11:44.477923 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 199, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:11:46.922384 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 199, CIFAR-10 Batch 2:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:11:49.355997 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 199, CIFAR-10 Batch 3:  Training   loss: 0.0067, accuracy: 1.0\n",
      "2017-05-18 00:11:51.739239 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 199, CIFAR-10 Batch 4:  Training   loss: 0.0077, accuracy: 1.0\n",
      "2017-05-18 00:11:54.134065 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 199, CIFAR-10 Batch 5:  Training   loss: 0.0075, accuracy: 1.0\n",
      "2017-05-18 00:11:56.536081 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 200, CIFAR-10 Batch 1:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:11:58.968196 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 200, CIFAR-10 Batch 2:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:12:01.384411 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 200, CIFAR-10 Batch 3:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:12:03.804739 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 200, CIFAR-10 Batch 4:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:12:06.217117 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 200, CIFAR-10 Batch 5:  Training   loss: 0.0075, accuracy: 1.0\n",
      "2017-05-18 00:12:08.655290 Validation loss: 0.94, accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:12:11.035566 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 201, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:12:13.460360 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 201, CIFAR-10 Batch 3:  Training   loss: 0.0067, accuracy: 1.0\n",
      "2017-05-18 00:12:15.871623 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 201, CIFAR-10 Batch 4:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:12:18.325914 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 201, CIFAR-10 Batch 5:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:12:20.755592 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 202, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:12:23.171904 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 202, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:12:25.592775 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 202, CIFAR-10 Batch 3:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:12:28.017691 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 202, CIFAR-10 Batch 4:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:12:30.435895 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 202, CIFAR-10 Batch 5:  Training   loss: 0.009, accuracy: 1.0\n",
      "2017-05-18 00:12:32.882556 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 203, CIFAR-10 Batch 1:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:12:35.299737 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 203, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:12:37.730992 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 203, CIFAR-10 Batch 3:  Training   loss: 0.0071, accuracy: 1.0\n",
      "2017-05-18 00:12:40.180257 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 203, CIFAR-10 Batch 4:  Training   loss: 0.0077, accuracy: 1.0\n",
      "2017-05-18 00:12:42.609715 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 203, CIFAR-10 Batch 5:  Training   loss: 0.0098, accuracy: 1.0\n",
      "2017-05-18 00:12:45.002329 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 204, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:12:47.461779 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 204, CIFAR-10 Batch 2:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:12:49.887026 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 204, CIFAR-10 Batch 3:  Training   loss: 0.0071, accuracy: 1.0\n",
      "2017-05-18 00:12:52.343682 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 204, CIFAR-10 Batch 4:  Training   loss: 0.007, accuracy: 1.0\n",
      "2017-05-18 00:12:54.765153 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 204, CIFAR-10 Batch 5:  Training   loss: 0.0077, accuracy: 1.0\n",
      "2017-05-18 00:12:57.172755 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 205, CIFAR-10 Batch 1:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:12:59.597049 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 205, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:13:02.007101 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 205, CIFAR-10 Batch 3:  Training   loss: 0.0057, accuracy: 1.0\n",
      "2017-05-18 00:13:04.398414 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 205, CIFAR-10 Batch 4:  Training   loss: 0.0075, accuracy: 1.0\n",
      "2017-05-18 00:13:06.792133 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 205, CIFAR-10 Batch 5:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:13:09.189221 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 206, CIFAR-10 Batch 1:  Training   loss: 0.0094, accuracy: 1.0\n",
      "2017-05-18 00:13:11.569714 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 206, CIFAR-10 Batch 2:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:13:13.934009 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 206, CIFAR-10 Batch 3:  Training   loss: 0.0072, accuracy: 1.0\n",
      "2017-05-18 00:13:16.332319 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 206, CIFAR-10 Batch 4:  Training   loss: 0.0076, accuracy: 1.0\n",
      "2017-05-18 00:13:18.753295 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 206, CIFAR-10 Batch 5:  Training   loss: 0.0077, accuracy: 1.0\n",
      "2017-05-18 00:13:21.157005 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 207, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:13:23.551777 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 207, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:13:25.938322 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 207, CIFAR-10 Batch 3:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:13:28.332648 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 207, CIFAR-10 Batch 4:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:13:30.710603 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 207, CIFAR-10 Batch 5:  Training   loss: 0.008, accuracy: 1.0\n",
      "2017-05-18 00:13:33.110660 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 208, CIFAR-10 Batch 1:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:13:35.476086 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 208, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:13:37.867380 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 208, CIFAR-10 Batch 3:  Training   loss: 0.0069, accuracy: 1.0\n",
      "2017-05-18 00:13:40.284362 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 208, CIFAR-10 Batch 4:  Training   loss: 0.0073, accuracy: 1.0\n",
      "2017-05-18 00:13:42.661680 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 208, CIFAR-10 Batch 5:  Training   loss: 0.0069, accuracy: 1.0\n",
      "2017-05-18 00:13:45.060091 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 209, CIFAR-10 Batch 1:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:13:47.454941 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 209, CIFAR-10 Batch 2:  Training   loss: 0.0093, accuracy: 1.0\n",
      "2017-05-18 00:13:49.844627 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 209, CIFAR-10 Batch 3:  Training   loss: 0.0067, accuracy: 1.0\n",
      "2017-05-18 00:13:52.249133 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 209, CIFAR-10 Batch 4:  Training   loss: 0.0087, accuracy: 1.0\n",
      "2017-05-18 00:13:54.662155 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 209, CIFAR-10 Batch 5:  Training   loss: 0.0076, accuracy: 1.0\n",
      "2017-05-18 00:13:57.093914 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 210, CIFAR-10 Batch 1:  Training   loss: 0.0095, accuracy: 1.0\n",
      "2017-05-18 00:13:59.513865 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 210, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:14:01.917221 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 210, CIFAR-10 Batch 3:  Training   loss: 0.0072, accuracy: 1.0\n",
      "2017-05-18 00:14:04.299711 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 210, CIFAR-10 Batch 4:  Training   loss: 0.0084, accuracy: 1.0\n",
      "2017-05-18 00:14:06.681894 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 210, CIFAR-10 Batch 5:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:14:09.067708 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 211, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:14:11.444434 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 211, CIFAR-10 Batch 2:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:14:13.831212 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 211, CIFAR-10 Batch 3:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:14:16.242585 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 211, CIFAR-10 Batch 4:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:14:18.625771 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 211, CIFAR-10 Batch 5:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:14:21.042855 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 212, CIFAR-10 Batch 1:  Training   loss: 0.0093, accuracy: 1.0\n",
      "2017-05-18 00:14:23.456612 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 212, CIFAR-10 Batch 2:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:14:25.864323 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 212, CIFAR-10 Batch 3:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:14:28.255084 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 212, CIFAR-10 Batch 4:  Training   loss: 0.0067, accuracy: 1.0\n",
      "2017-05-18 00:14:30.666241 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 212, CIFAR-10 Batch 5:  Training   loss: 0.0074, accuracy: 1.0\n",
      "2017-05-18 00:14:33.063982 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 213, CIFAR-10 Batch 1:  Training   loss: 0.0098, accuracy: 1.0\n",
      "2017-05-18 00:14:35.429645 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 213, CIFAR-10 Batch 2:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:14:37.800823 Validation loss: 0.94, accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213, CIFAR-10 Batch 3:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:14:40.169213 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 213, CIFAR-10 Batch 4:  Training   loss: 0.0077, accuracy: 1.0\n",
      "2017-05-18 00:14:42.585957 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 213, CIFAR-10 Batch 5:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:14:45.000832 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 214, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:14:47.398890 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 214, CIFAR-10 Batch 2:  Training   loss: 0.0095, accuracy: 1.0\n",
      "2017-05-18 00:14:49.816658 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 214, CIFAR-10 Batch 3:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:14:52.189056 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 214, CIFAR-10 Batch 4:  Training   loss: 0.0076, accuracy: 1.0\n",
      "2017-05-18 00:14:54.588774 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 214, CIFAR-10 Batch 5:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:14:56.947921 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 215, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:14:59.312142 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 215, CIFAR-10 Batch 2:  Training   loss: 0.0098, accuracy: 1.0\n",
      "2017-05-18 00:15:01.675043 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 215, CIFAR-10 Batch 3:  Training   loss: 0.0059, accuracy: 1.0\n",
      "2017-05-18 00:15:04.083436 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 215, CIFAR-10 Batch 4:  Training   loss: 0.0071, accuracy: 1.0\n",
      "2017-05-18 00:15:06.490834 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 215, CIFAR-10 Batch 5:  Training   loss: 0.0069, accuracy: 1.0\n",
      "2017-05-18 00:15:08.870937 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 216, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:15:11.282302 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 216, CIFAR-10 Batch 2:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:15:13.686365 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 216, CIFAR-10 Batch 3:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:15:16.080196 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 216, CIFAR-10 Batch 4:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:15:18.477093 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 216, CIFAR-10 Batch 5:  Training   loss: 0.0093, accuracy: 1.0\n",
      "2017-05-18 00:15:20.887169 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 217, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:15:23.323000 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 217, CIFAR-10 Batch 2:  Training   loss: 0.011, accuracy: 1.0\n",
      "2017-05-18 00:15:25.705226 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 217, CIFAR-10 Batch 3:  Training   loss: 0.0073, accuracy: 1.0\n",
      "2017-05-18 00:15:28.096693 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 217, CIFAR-10 Batch 4:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:15:30.514581 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 217, CIFAR-10 Batch 5:  Training   loss: 0.0073, accuracy: 1.0\n",
      "2017-05-18 00:15:32.908525 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 218, CIFAR-10 Batch 1:  Training   loss: 0.0091, accuracy: 1.0\n",
      "2017-05-18 00:15:35.331156 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 218, CIFAR-10 Batch 2:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:15:37.743609 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 218, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:15:40.163945 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 218, CIFAR-10 Batch 4:  Training   loss: 0.0093, accuracy: 1.0\n",
      "2017-05-18 00:15:42.584728 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 218, CIFAR-10 Batch 5:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:15:45.026763 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 219, CIFAR-10 Batch 1:  Training   loss: 0.0093, accuracy: 1.0\n",
      "2017-05-18 00:15:47.453969 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 219, CIFAR-10 Batch 2:  Training   loss: 0.0094, accuracy: 1.0\n",
      "2017-05-18 00:15:49.890729 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 219, CIFAR-10 Batch 3:  Training   loss: 0.008, accuracy: 1.0\n",
      "2017-05-18 00:15:52.354870 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 219, CIFAR-10 Batch 4:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:15:54.768649 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 219, CIFAR-10 Batch 5:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:15:57.180054 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 220, CIFAR-10 Batch 1:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:15:59.618309 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 220, CIFAR-10 Batch 2:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:16:02.031197 Validation loss: 0.96, accuracy: 0.73\n",
      "Epoch 220, CIFAR-10 Batch 3:  Training   loss: 0.0058, accuracy: 1.0\n",
      "2017-05-18 00:16:04.466358 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 220, CIFAR-10 Batch 4:  Training   loss: 0.0069, accuracy: 1.0\n",
      "2017-05-18 00:16:06.889242 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 220, CIFAR-10 Batch 5:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:16:09.344396 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 221, CIFAR-10 Batch 1:  Training   loss: 0.008, accuracy: 1.0\n",
      "2017-05-18 00:16:11.777761 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 221, CIFAR-10 Batch 2:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:16:14.201728 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 221, CIFAR-10 Batch 3:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:16:16.605498 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 221, CIFAR-10 Batch 4:  Training   loss: 0.0064, accuracy: 1.0\n",
      "2017-05-18 00:16:19.024471 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 221, CIFAR-10 Batch 5:  Training   loss: 0.0076, accuracy: 1.0\n",
      "2017-05-18 00:16:21.439868 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 222, CIFAR-10 Batch 1:  Training   loss: 0.008, accuracy: 1.0\n",
      "2017-05-18 00:16:23.836941 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 222, CIFAR-10 Batch 2:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:16:26.249967 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 222, CIFAR-10 Batch 3:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:16:28.658583 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 222, CIFAR-10 Batch 4:  Training   loss: 0.0075, accuracy: 1.0\n",
      "2017-05-18 00:16:31.065307 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 222, CIFAR-10 Batch 5:  Training   loss: 0.0077, accuracy: 1.0\n",
      "2017-05-18 00:16:33.408083 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 223, CIFAR-10 Batch 1:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:16:35.795380 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 223, CIFAR-10 Batch 2:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:16:38.177024 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 223, CIFAR-10 Batch 3:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:16:40.548471 Validation loss: 0.93, accuracy: 0.73\n",
      "Epoch 223, CIFAR-10 Batch 4:  Training   loss: 0.0069, accuracy: 1.0\n",
      "2017-05-18 00:16:42.957831 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 223, CIFAR-10 Batch 5:  Training   loss: 0.009, accuracy: 1.0\n",
      "2017-05-18 00:16:45.342438 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 224, CIFAR-10 Batch 1:  Training   loss: 0.0087, accuracy: 1.0\n",
      "2017-05-18 00:16:47.745958 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 224, CIFAR-10 Batch 2:  Training   loss: 0.0084, accuracy: 1.0\n",
      "2017-05-18 00:16:50.148562 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 224, CIFAR-10 Batch 3:  Training   loss: 0.0054, accuracy: 1.0\n",
      "2017-05-18 00:16:52.540989 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 224, CIFAR-10 Batch 4:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:16:54.901466 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 224, CIFAR-10 Batch 5:  Training   loss: 0.0073, accuracy: 1.0\n",
      "2017-05-18 00:16:57.301711 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 225, CIFAR-10 Batch 1:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:16:59.696091 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 225, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:17:02.098140 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 225, CIFAR-10 Batch 3:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:17:04.510890 Validation loss: 0.93, accuracy: 0.74\n",
      "Epoch 225, CIFAR-10 Batch 4:  Training   loss: 0.0073, accuracy: 1.0\n",
      "2017-05-18 00:17:06.930068 Validation loss: 0.95, accuracy: 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225, CIFAR-10 Batch 5:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:17:09.325960 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 226, CIFAR-10 Batch 1:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:17:11.722448 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 226, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:17:14.119110 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 226, CIFAR-10 Batch 3:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:17:16.505485 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 226, CIFAR-10 Batch 4:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:17:18.912550 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 226, CIFAR-10 Batch 5:  Training   loss: 0.0064, accuracy: 1.0\n",
      "2017-05-18 00:17:21.320894 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 227, CIFAR-10 Batch 1:  Training   loss: 0.0096, accuracy: 1.0\n",
      "2017-05-18 00:17:23.735497 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 227, CIFAR-10 Batch 2:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:17:26.124655 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 227, CIFAR-10 Batch 3:  Training   loss: 0.0064, accuracy: 1.0\n",
      "2017-05-18 00:17:28.533468 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 227, CIFAR-10 Batch 4:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:17:30.941308 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 227, CIFAR-10 Batch 5:  Training   loss: 0.0064, accuracy: 1.0\n",
      "2017-05-18 00:17:33.335220 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 228, CIFAR-10 Batch 1:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:17:35.724222 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 228, CIFAR-10 Batch 2:  Training   loss: 0.0094, accuracy: 1.0\n",
      "2017-05-18 00:17:38.121098 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 228, CIFAR-10 Batch 3:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:17:40.500288 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 228, CIFAR-10 Batch 4:  Training   loss: 0.0066, accuracy: 1.0\n",
      "2017-05-18 00:17:42.909364 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 228, CIFAR-10 Batch 5:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:17:45.307991 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 229, CIFAR-10 Batch 1:  Training   loss: 0.0095, accuracy: 1.0\n",
      "2017-05-18 00:17:47.704247 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 229, CIFAR-10 Batch 2:  Training   loss: 0.0099, accuracy: 1.0\n",
      "2017-05-18 00:17:50.083398 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 229, CIFAR-10 Batch 3:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:17:52.500097 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 229, CIFAR-10 Batch 4:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:17:54.881168 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 229, CIFAR-10 Batch 5:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:17:57.284687 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 230, CIFAR-10 Batch 1:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:17:59.682488 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 230, CIFAR-10 Batch 2:  Training   loss: 0.01, accuracy: 1.0\n",
      "2017-05-18 00:18:02.069503 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 230, CIFAR-10 Batch 3:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:18:04.451497 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 230, CIFAR-10 Batch 4:  Training   loss: 0.007, accuracy: 1.0\n",
      "2017-05-18 00:18:06.846133 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 230, CIFAR-10 Batch 5:  Training   loss: 0.0084, accuracy: 1.0\n",
      "2017-05-18 00:18:09.244457 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 231, CIFAR-10 Batch 1:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:18:11.631000 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 231, CIFAR-10 Batch 2:  Training   loss: 0.0097, accuracy: 1.0\n",
      "2017-05-18 00:18:14.021136 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 231, CIFAR-10 Batch 3:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:18:16.417557 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 231, CIFAR-10 Batch 4:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:18:18.807826 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 231, CIFAR-10 Batch 5:  Training   loss: 0.0075, accuracy: 1.0\n",
      "2017-05-18 00:18:21.187928 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 232, CIFAR-10 Batch 1:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:18:23.600602 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 232, CIFAR-10 Batch 2:  Training   loss: 0.0092, accuracy: 1.0\n",
      "2017-05-18 00:18:26.002794 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 232, CIFAR-10 Batch 3:  Training   loss: 0.0062, accuracy: 1.0\n",
      "2017-05-18 00:18:28.392630 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 232, CIFAR-10 Batch 4:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:18:30.769790 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 232, CIFAR-10 Batch 5:  Training   loss: 0.0068, accuracy: 1.0\n",
      "2017-05-18 00:18:33.160065 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 233, CIFAR-10 Batch 1:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:18:35.569817 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 233, CIFAR-10 Batch 2:  Training   loss: 0.0091, accuracy: 1.0\n",
      "2017-05-18 00:18:37.956465 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 233, CIFAR-10 Batch 3:  Training   loss: 0.0057, accuracy: 1.0\n",
      "2017-05-18 00:18:40.360311 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 233, CIFAR-10 Batch 4:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:18:42.740227 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 233, CIFAR-10 Batch 5:  Training   loss: 0.0073, accuracy: 1.0\n",
      "2017-05-18 00:18:45.167274 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 234, CIFAR-10 Batch 1:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:18:47.564147 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 234, CIFAR-10 Batch 2:  Training   loss: 0.0087, accuracy: 1.0\n",
      "2017-05-18 00:18:49.952061 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 234, CIFAR-10 Batch 3:  Training   loss: 0.0058, accuracy: 1.0\n",
      "2017-05-18 00:18:52.342697 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 234, CIFAR-10 Batch 4:  Training   loss: 0.0059, accuracy: 1.0\n",
      "2017-05-18 00:18:54.762493 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 234, CIFAR-10 Batch 5:  Training   loss: 0.007, accuracy: 1.0\n",
      "2017-05-18 00:18:57.131354 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 235, CIFAR-10 Batch 1:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:18:59.531440 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 235, CIFAR-10 Batch 2:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:19:01.920446 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 235, CIFAR-10 Batch 3:  Training   loss: 0.0053, accuracy: 1.0\n",
      "2017-05-18 00:19:04.335712 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 235, CIFAR-10 Batch 4:  Training   loss: 0.0064, accuracy: 1.0\n",
      "2017-05-18 00:19:06.750171 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 235, CIFAR-10 Batch 5:  Training   loss: 0.0076, accuracy: 1.0\n",
      "2017-05-18 00:19:09.144741 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 236, CIFAR-10 Batch 1:  Training   loss: 0.009, accuracy: 1.0\n",
      "2017-05-18 00:19:11.567657 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 236, CIFAR-10 Batch 2:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:19:13.965308 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 236, CIFAR-10 Batch 3:  Training   loss: 0.0057, accuracy: 1.0\n",
      "2017-05-18 00:19:16.363675 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 236, CIFAR-10 Batch 4:  Training   loss: 0.0061, accuracy: 1.0\n",
      "2017-05-18 00:19:18.766850 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 236, CIFAR-10 Batch 5:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:19:21.164389 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 237, CIFAR-10 Batch 1:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:19:23.576391 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 237, CIFAR-10 Batch 2:  Training   loss: 0.0089, accuracy: 1.0\n",
      "2017-05-18 00:19:25.997119 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 237, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:19:28.398994 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 237, CIFAR-10 Batch 4:  Training   loss: 0.006, accuracy: 1.0\n",
      "2017-05-18 00:19:30.829241 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 237, CIFAR-10 Batch 5:  Training   loss: 0.006, accuracy: 1.0\n",
      "2017-05-18 00:19:33.256560 Validation loss: 0.94, accuracy: 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238, CIFAR-10 Batch 1:  Training   loss: 0.0084, accuracy: 1.0\n",
      "2017-05-18 00:19:35.673059 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 238, CIFAR-10 Batch 2:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:19:38.059158 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 238, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:19:40.471605 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 238, CIFAR-10 Batch 4:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:19:42.857383 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 238, CIFAR-10 Batch 5:  Training   loss: 0.0067, accuracy: 1.0\n",
      "2017-05-18 00:19:45.258077 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 239, CIFAR-10 Batch 1:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:19:47.613830 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 239, CIFAR-10 Batch 2:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:19:50.015007 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 239, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:19:52.401652 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 239, CIFAR-10 Batch 4:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:19:54.772915 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 239, CIFAR-10 Batch 5:  Training   loss: 0.007, accuracy: 1.0\n",
      "2017-05-18 00:19:57.204885 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 240, CIFAR-10 Batch 1:  Training   loss: 0.0079, accuracy: 1.0\n",
      "2017-05-18 00:19:59.583136 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 240, CIFAR-10 Batch 2:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:20:01.962300 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 240, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:20:04.357596 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 240, CIFAR-10 Batch 4:  Training   loss: 0.0057, accuracy: 1.0\n",
      "2017-05-18 00:20:06.743267 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 240, CIFAR-10 Batch 5:  Training   loss: 0.0066, accuracy: 1.0\n",
      "2017-05-18 00:20:09.099151 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 241, CIFAR-10 Batch 1:  Training   loss: 0.0084, accuracy: 1.0\n",
      "2017-05-18 00:20:11.540907 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 241, CIFAR-10 Batch 2:  Training   loss: 0.0078, accuracy: 1.0\n",
      "2017-05-18 00:20:13.958091 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 241, CIFAR-10 Batch 3:  Training   loss: 0.0053, accuracy: 1.0\n",
      "2017-05-18 00:20:16.418620 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 241, CIFAR-10 Batch 4:  Training   loss: 0.0057, accuracy: 1.0\n",
      "2017-05-18 00:20:18.803729 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 241, CIFAR-10 Batch 5:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:20:21.183151 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 242, CIFAR-10 Batch 1:  Training   loss: 0.0088, accuracy: 1.0\n",
      "2017-05-18 00:20:23.582143 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 242, CIFAR-10 Batch 2:  Training   loss: 0.0086, accuracy: 1.0\n",
      "2017-05-18 00:20:26.020447 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 242, CIFAR-10 Batch 3:  Training   loss: 0.0052, accuracy: 1.0\n",
      "2017-05-18 00:20:28.419695 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 242, CIFAR-10 Batch 4:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:20:30.815499 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 242, CIFAR-10 Batch 5:  Training   loss: 0.0063, accuracy: 1.0\n",
      "2017-05-18 00:20:33.250326 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 243, CIFAR-10 Batch 1:  Training   loss: 0.0084, accuracy: 1.0\n",
      "2017-05-18 00:20:35.630349 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 243, CIFAR-10 Batch 2:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:20:38.027821 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 243, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:20:40.444639 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 243, CIFAR-10 Batch 4:  Training   loss: 0.006, accuracy: 1.0\n",
      "2017-05-18 00:20:42.834540 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 243, CIFAR-10 Batch 5:  Training   loss: 0.0064, accuracy: 1.0\n",
      "2017-05-18 00:20:45.202741 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 244, CIFAR-10 Batch 1:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:20:47.560619 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 244, CIFAR-10 Batch 2:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:20:49.951195 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 244, CIFAR-10 Batch 3:  Training   loss: 0.0055, accuracy: 1.0\n",
      "2017-05-18 00:20:52.347125 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 244, CIFAR-10 Batch 4:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:20:54.747904 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 244, CIFAR-10 Batch 5:  Training   loss: 0.0066, accuracy: 1.0\n",
      "2017-05-18 00:20:57.148447 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 245, CIFAR-10 Batch 1:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:20:59.540042 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 245, CIFAR-10 Batch 2:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:21:01.926489 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 245, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:21:04.331017 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 245, CIFAR-10 Batch 4:  Training   loss: 0.0058, accuracy: 1.0\n",
      "2017-05-18 00:21:06.723444 Validation loss: 0.94, accuracy: 0.73\n",
      "Epoch 245, CIFAR-10 Batch 5:  Training   loss: 0.0064, accuracy: 1.0\n",
      "2017-05-18 00:21:09.121143 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 246, CIFAR-10 Batch 1:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:21:11.551598 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 246, CIFAR-10 Batch 2:  Training   loss: 0.0087, accuracy: 1.0\n",
      "2017-05-18 00:21:13.953936 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 246, CIFAR-10 Batch 3:  Training   loss: 0.0055, accuracy: 1.0\n",
      "2017-05-18 00:21:16.336167 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 246, CIFAR-10 Batch 4:  Training   loss: 0.0057, accuracy: 1.0\n",
      "2017-05-18 00:21:18.721770 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 246, CIFAR-10 Batch 5:  Training   loss: 0.0058, accuracy: 1.0\n",
      "2017-05-18 00:21:21.132999 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 247, CIFAR-10 Batch 1:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:21:23.566229 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 247, CIFAR-10 Batch 2:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:21:25.956218 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 247, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:21:28.350255 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 247, CIFAR-10 Batch 4:  Training   loss: 0.0054, accuracy: 1.0\n",
      "2017-05-18 00:21:30.799163 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 247, CIFAR-10 Batch 5:  Training   loss: 0.0059, accuracy: 1.0\n",
      "2017-05-18 00:21:33.201588 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 248, CIFAR-10 Batch 1:  Training   loss: 0.0081, accuracy: 1.0\n",
      "2017-05-18 00:21:35.612019 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 248, CIFAR-10 Batch 2:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:21:38.021231 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 248, CIFAR-10 Batch 3:  Training   loss: 0.0055, accuracy: 1.0\n",
      "2017-05-18 00:21:40.450115 Validation loss: 0.94, accuracy: 0.74\n",
      "Epoch 248, CIFAR-10 Batch 4:  Training   loss: 0.0051, accuracy: 1.0\n",
      "2017-05-18 00:21:42.850129 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 248, CIFAR-10 Batch 5:  Training   loss: 0.0065, accuracy: 1.0\n",
      "2017-05-18 00:21:45.242240 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 249, CIFAR-10 Batch 1:  Training   loss: 0.0082, accuracy: 1.0\n",
      "2017-05-18 00:21:47.621797 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 249, CIFAR-10 Batch 2:  Training   loss: 0.0084, accuracy: 1.0\n",
      "2017-05-18 00:21:50.042273 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 249, CIFAR-10 Batch 3:  Training   loss: 0.0054, accuracy: 1.0\n",
      "2017-05-18 00:21:52.471671 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 249, CIFAR-10 Batch 4:  Training   loss: 0.0051, accuracy: 1.0\n",
      "2017-05-18 00:21:54.888768 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 249, CIFAR-10 Batch 5:  Training   loss: 0.006, accuracy: 1.0\n",
      "2017-05-18 00:21:57.296304 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 250, CIFAR-10 Batch 1:  Training   loss: 0.0083, accuracy: 1.0\n",
      "2017-05-18 00:21:59.722071 Validation loss: 0.95, accuracy: 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250, CIFAR-10 Batch 2:  Training   loss: 0.0085, accuracy: 1.0\n",
      "2017-05-18 00:22:02.101452 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 250, CIFAR-10 Batch 3:  Training   loss: 0.0056, accuracy: 1.0\n",
      "2017-05-18 00:22:04.508050 Validation loss: 0.95, accuracy: 0.74\n",
      "Epoch 250, CIFAR-10 Batch 4:  Training   loss: 0.0058, accuracy: 1.0\n",
      "2017-05-18 00:22:06.905863 Validation loss: 0.95, accuracy: 0.73\n",
      "Epoch 250, CIFAR-10 Batch 5:  Training   loss: 0.0058, accuracy: 1.0\n",
      "2017-05-18 00:22:09.322609 Validation loss: 0.95, accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "full_val_accuracy = []\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels, epoch)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            full_val_accuracy.append(print_stats(sess, batch_features, batch_labels, cost, accuracy))\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9616bd65f8>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAv4AAALlCAYAAAC8dOLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAOgA4Q8AAB3w/wOGPpjwTDNmEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f961f731cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "fig, axis = plt.subplots(figsize=(13,13))\n",
    "axis.plot(np.array(range(len(full_val_accuracy)))/5, full_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.728515625\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9604674710>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "        \n",
    "#         file_writer = tf.summary.FileWriter('tensorboard', sess.graph)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch,\n",
    "                           loaded_y: test_label_batch,\n",
    "                           loaded_keep_prob:  1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
